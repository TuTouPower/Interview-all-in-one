# 二、HTTP

## 2.1 HTTP 常⻅面试题

## HTTP 基本概念

#### HTTP 是什么？描述一下

HTTP 是超文本传输协议，也就是 **H** yperText **T** ransfer **P** rotocol。

```
能否详细解释「超文本传输协议」？
```

HTTP的名字「超文本协议传输」，它可以拆成三个部分：
超文本
传输
协议


#### 1. 「协议」

#### 在生活中，我们也能随处可⻅「协议」，例如：

#### 刚毕业时会签一个「三方协议」；

#### 找房子时会签一个「租房协议」；

#### 生活中的协议，本质上与计算机中的协议是相同的，协议的特点:

#### 「个；租房协议里的参与者有两个：你和房东。 协 」字，代表的意思是必须有 两个以上的参与者 。例如三方协议里的参与者有三个：你、公司、学校三

#### 「房协议里规定租期期限、每月租金金额、违约如何处理等。 议 」字，代表的意思是对参与者的一种 行为约定和规范 。例如三方协议里规定试用期期限、毁约金等；租

#### 针对 HTTP 协议 ，我们可以这么理解。

#### HTTP 个以上的参与者 是一个用在计算机世界里的），以及相关的各种控制和错误处理方式（ 协议 。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范（ 行为约定和规范 ）。 两

#### 2. 「传输」


#### 所谓的「传输」，很好理解，就是把一堆东⻄从 A 点搬到 B 点，或者从 B 点 搬到 A 点。

#### 别轻视了这个简单的动作，它至少包含两项重要的信息。

#### HTTP 协议是一个 双向协议 。

#### 我们在上网冲浪时，浏览器是请求方求数据发送给网站，网站再把一些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图片、视频了。 A ，百度网站就是应答方 B。双方约定用 HTTP 协议来通信，于是浏览器把请

#### 数据虽然是在 A 和 B 之间传输，但允许中间有 中转或接力 。

#### 就好像第一排的同学想传递纸条给最后一排的同学，那么传递的过程中就需要经过好多个同学（中间人），这样的传输方式就从「A < --- > B」，变成了「A <-> N <-> M <-> B」。

#### 而在 HTTP 里，需要中间人遵从 HTTP 协议，只要不打扰基本的数据传输，就可以添加任意额外的东⻄。

#### 针对 传输 ，我们可以进一步理解了 HTTP。

#### HTTP 是一个在计算机世界里专⻔用来在 两点之间传输数据 的约定和规范。

#### 3. 「超文本」

#### HTTP 传输的内容是「超文本」。

#### 我们先来理解「文本」，在互联网早期的时候只是简单的字符文字，但现在「文本」的涵义已经可以扩展为图片、视频、压缩包等，在 HTTP 眼里这些都算作「文本」。

#### 再来理解「超文本」，它就是一个超文本跳转到另外一个超文本。 超越了普通文本的文本 ，它是文字、图片、视频等的混合体，最关键有超链接，能从

#### HTML 览器的解释，呈现给我们的就是一个文字、有画面的网⻚了。就是最常⻅的超文本了，它本身只是纯文字文件，但内部用很多标签定义了图片、视频等的链接，再经过浏


#### OK的答案：，经过了对 HTTP 里这三个名词的详细解释，就可以给出比「超文本传输协议」这七个字更准确更有技术含量

#### HTTP 规范」。是一个在计算机世界里专⻔在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和

#### 那「HTTP 是用于从互联网服务器传输超文本到本地浏览器的协议 ，这种说法正确吗？

#### 这种说法是 不正确 的。因为也可以是「服务器< -- >服务器」，所以采用 两点之间 的描述会更准确。

#### HTTP 常⻅的状态码，有哪些？

_1xx_
1xx 类状态码属于 **提示信息** ，是协议处理中的一种中间状态，实际用到的比较少。
_2xx_
2xx 类状态码表示服务器 **成功** 处理了客户端的请求，也是我们最愿意看到的状态。
「数据。 **200 OK** 」是最常⻅的成功状态码，表示一切正常。如果是非 HEAD 请求，服务器返回的响应头都会有 body

「 **204 No Content** 」也是常⻅的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。


「是其中的一部分，也是服务器处理成功的状态。 **206 Partial Content** 」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而

_3xx_
3xx 类状态码表示客户端请求的资源发送了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是 **重定
向** 。
「 **301 Moved Permanently** 」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。
「 **302 Found** 」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。
301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。

「存控制。 **304 Not Modified** 」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，用于缓

_4xx_
4xx 类状态码表示客户端发送的 **报文有误** ，服务器无法处理，也就是错误码的含义。
「 **400 Bad Request** 」表示客户端请求的报文有错误，但只是个笼统的错误。
「 **403 Forbidden** 」表示服务器禁止访问资源，并不是客户端的请求出错。
「 **404 Not Found** 」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。
_5xx_
5xx 类状态码表示客户端请求报文正确，但是 **服务器处理时内部发生了错误** ，属于服务器端的错误码。
「 **500 Internal Server Error** 」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。
「 **501 Not Implemented** 」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。

「发生了错误。 **502 Bad Gateway** 」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器

「思。 **503 Service Unavailable** 」表示服务器当前很忙，暂时无法响应服务器，类似“网络服务正忙，请稍后重试”的意

```
http 常⻅字段有哪些？
```

_Host_ 字段


#### 客户端发送请求时，用来指定服务器的域名。

有了 Host 字段，就可以将请求发往「同一台」服务器上的不同网站。
_Content-Length_ 字段
服务器在返回数据时，会有 Content-Length 字段，表明本次回应的数据⻓度。

#### 如上面则是告诉浏览器，本次服务器回应的数据⻓度是 1000 个字节，后面的字节就属于下一个回应了。

_Connection_ 字段

```
Host: http://www.A.com
```

```
Content-Length: 1000
```

```
Connection 字段最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用。
```

HTTP/1.1 Keep-Alive版本的默认连接都是持久连接，但为了兼容老版本的。 HTTP，需要指定 Connection 首部字段的值为

#### 一个可以复用的 TCP 连接就建立了，直到客户端或服务器主动关闭连接。但是，这不是标准字段。

_Content-Type_ 字段
Content-Type 字段用于服务器回应时，告诉客户端，本次数据是什么格式。

#### 上面的类型表明，发送的是网⻚，而且编码是UTF-8。

```
Connection: keep-alive
```

```
Content-Type: text/html; charset=utf-
```

客户端请求的时候，可以使用 Accept 字段声明自己可以接受哪些数据格式。

上面代码中，客户端声明自己可以接受任何格式的数据。
_Content-Encoding_ 字段
Content-Encoding 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式

上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。
客户端在请求时，用 Accept-Encoding 字段说明自己可以接受哪些压缩方法。

## GET 与 POST^

#### 说一下 GET 和 POST 的区别？

Get 方法的含义是请求 **从服务器获取资源** ，这个资源可以是静态的文本、⻚面、图片视频等。
比如，你打开我的文章，浏览器就会发送 GET 请求给服务器，服务器就会返回文章的所有文字及资源。

```
Accept: */*
```

```
Content-Encoding: gzip
```

```
Accept-Encoding: gzip, deflate
```

而POST 方法则是相反操作，它向 URI 指定的资源提交数据，数据就放在报文的 body 里。
比如，你在我文章底部，敲入了留言后点击「提交」（留言文字放进了报文 body 里，然后拼接好 POST 请求头，通过 **暗示你们留言** TCP ），浏览器就会执行一次协议发送给服务器。 POST 请求，把你的

#### GET 和 POST 方法都是安全和幂等的吗？

#### 先说明下安全和幂等的概念：

#### 在 HTTP 协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。

#### 所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。

#### 那么很明显的，且每次的结果都是相同的。 GET 方法就是安全且幂等的 ，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全


#### POST 个资源，所以 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是 不是幂等 的。 不安全 的，且多次提交数据就会创建多

## HTTP 特性^

#### 你知道的 HTTP（1.1） 的优点有哪些，怎么体现的？

#### HTTP 最凸出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。

#### 1. 简单

HTTP 和使用的⻔槛。基本的报文格式就是 header + body，头部信息也是 key-value 简单文本的形式， **易于理解** ，降低了学习

_2._ 灵活和易于扩展
HTTP **义和扩充** 协议里的各类请求方法、。 URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员 **自定**

同时 HTTP 由于是工作在应用层（ OSI 第七层），则它 **下层可以随意变化** 。
HTTPS QUIC。也就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层，HTTP/3 甚至把 TCP 层换成了基于 UDP 的

_3._ 应用广泛和跨平台
互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购
物、理财、吃鸡，HTTP 的应用 **片地开花** ，同时天然具有 **跨平台** 的优越性。

```
那它的缺点呢？
```

HTTP 协议里有优缺点一体的 **双刃剑** ，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。

_1._ 无状态双刃剑
无状态的负担，能够把更多的 **好处** ，因为服务器不会去记忆 CPU 和内存用来对外提供服务。 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的

无状态的 **坏处** ，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。
例如登录关联的，每次都要问一遍身份信息。->添加购物⻋->下单->结算->支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有


#### 这样每操作一次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是 酸爽 ！

对于无状态的问题，解法方案有很多种，其中比较简单的方式用 **Cookie** 技术。
Cookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。
相当于， **带上「小贴纸」，服务器就能认得了了在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，** ，

#### 2. 明文传输双刃剑

明文意味着在传输过程中的信息，是可方便阅读的，通过浏览器的看，为我们调试工作带了极大的便利性。 F12 控制台或 Wireshark 抓包都可以直接肉眼查


#### 但是这正是这样，容都毫无隐私可言，很容易就能被窃取，如果里面有你的账号密码信息，那HTTP 的所有信息都暴露在了光天化日下，相当于 信息裸奔你号没了 。在传输的漫⻓的过程中，信息的内。

#### 3. 不安全

#### HTTP 比较严重的缺点就是不安全：

#### 通信使用明文（不加密），内容可能会被窃听。比如， 账号信息容易泄漏，那你号没了。

#### 不验证通信方的身份，因此有可能遭遇伪装。比如， 访问假的淘宝、拼多多，那你钱没了。

#### 无法证明报文的完整性，所以有可能已遭篡改。比如， 网⻚上植入垃圾广告，视觉污染，眼没了。

#### HTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。

#### 那你再说下 HTTP/1.1 的性能如何？

#### HTTP 协议是基于 TCP/IP ，并且使用了「 请求 - 应答 」的通信模式，所以性能的关键就在这 两点 里。

#### 1. ⻓连接


#### 早期串行请求，做了无谓的 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接建立和断开，增加了通信开销。 TCP 连接（三次握手），而且是

#### 为了解决上述TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。 TCP 连接问题，HTTP/1.1 提出了 ⻓连接 的通信方式，也叫持久连接。这种方式的好处在于减少了

#### 持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。

#### 2. 管道网络传输

HTTP/1.1 采用了⻓连接的方式，这使得管道（pipeline）网络传输成为了可能。
即可在同一个二个请求出去，可以 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第 **减少整体的响应时间。**

举例来说，客户端需要请求两个资源。以前的做法是，在同一个出回应，收到后再发出 B 请求。管道机制则是允许浏览器同时发出TCP A 连接里面，先发送请求和 B 请求。 A 请求，然后等待服务器做


#### 但是服务器还是按照排队等着。这称为「队头堵塞」。 顺序 ，先回应 A 请求，完成后再回应 B 请求。要是前面的回应特别慢，后面就会有许多请求

#### 3. 队头阻塞

#### 「请求 - 应答」的模式加剧了 HTTP 的性能问题。

#### 因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致

#### 客户端一直请求不到数据，这也就是「 队头阻塞 」。 好比上班的路上塞⻋ 。


#### 总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。

## HTTP 与 HTTPS^

#### HTTP 与 HTTPS 有哪些区别？

#### 1. HTTP TCP 和是超文本传输协议，信息是明文传输，存在安全⻛险的问题。 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。HTTPS 则解决 HTTP 不安全的缺陷，在

#### 2. HTTP 后，还需进行连接建立相对简单， SSL/TLS 的握手过程，才可进入加密报文传输。 TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之

#### 3. HTTP 的端口号是 80 ，HTTPS 的端口号是 443 。

#### 4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

#### HTTPS 解决了 HTTP 的哪些问题？

#### HTTP 由于是明文传输，所以安全上存在以下三个⻛险：

#### 窃听⻛险 ，比如通信链路上可以获取通信内容，用户号容易没。

#### 篡改⻛险 ，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。


#### 冒充⻛险 ，比如冒充淘宝网站，用户钱容易没。

#### HTTP S 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的⻛险：

#### 信息加密 ：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。

#### 校验机制 ：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。

#### 身份证书 ：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。

#### 可⻅，只要自身不做「恶」，SSL/TLS 协议是能保证通信是安全的。

#### HTTPS 是如何解决上面的三个⻛险的？

#### 混合加密 的方式实现信息的 机密性 ，解决了窃听的⻛险。

#### 摘要算法 了篡改的⻛险。的方式来实现 完整性 ，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决

#### 将服务器公钥放入到 数字证书 中，解决了冒充的⻛险。

#### 1. 混合加密

#### 通过 混合加密 的方式可以保证信息的 机密性 ，解决了窃听的⻛险。


#### HTTPS 采用的是 对称加密 和 非对称加密 结合的「混合加密」方式：

#### 在通信建立前采用 非对称加密 的方式交换「会话秘钥」，后续就不再使用非对称加密。

#### 在通信过程中全部使用 对称加密 的「会话秘钥」的方式加密明文数据。

#### 采用「混合加密」的方式的原因：

#### 对称加密 只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。

#### 非对称加密 使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。

#### 2. 摘要算法

#### 摘要算法 用来实现 完整性 ，能够为数据生成独一无二的「指纹」，用于校验数据的完整性，解决了篡改的⻛险。


#### 客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出 + 明文」一同加密成密文后，发

#### 的「指纹」做比较，若「指纹」相同，说明数据是完整的。

#### 3. 数字证书

#### 客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。

#### 这就存在些问题，如何保证公钥不被篡改和信任度？

#### 所以这里就需要借助第三方权威机构机构颁发）中，只要证书是可信的，公钥就是可信的。 CA （数字证书认证机构），将 服务器公钥放在数字证书 （由数字证书认证


#### 通过数字证书的方式保证服务器公钥的身份，解决冒充的⻛险。

#### HTTPS 是如何建立连接的？其间交互了什么？

#### SSL/TLS 协议基本流程：

#### 客户端向服务器索要并验证服务器的公钥。

#### 双方协商生产「会话秘钥」。

#### 双方采用「会话秘钥」进行加密通信。

#### 前两步也就是 SSL/TLS 的建立过程，也就是握手阶段。

#### SSL/TLS 的「握手阶段」涉及 四次 通信，可⻅下图：



#### SSL/TLS 协议建立的详细流程：

_1. ClientHello_
首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。
在这一步，客户端主要向服务器发送以下信息：
（ 1 ）客户端支持的 SSL/TLS 协议版本，如 TLS 1.2 版本。
（ 2 ）客户端生产的随机数（Client Random），后面用于生产「会话秘钥」。
（ 3 ）客户端支持的密码套件列表，如 RSA 加密算法。
_2. SeverHello_
服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello。服务器回应的内容有如下内容：
（ 1 ）确认 SSL/ TLS 协议版本，如果浏览器不支持，则关闭加密通信。
（ 2 ）服务器生产的随机数（Server Random），后面用于生产「会话秘钥」。
（ 3 ）确认的密码套件列表，如 RSA 加密算法。
（ 4 ）服务器的数字证书。
_3._ 客户端回应
客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。
如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：
（ 1 ）一个随机数（pre-master key）。该随机数会被服务器公钥加密。
（ 2 ）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
（要，用来供服务端校验。 3 ）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘


#### 上面第一项的随机数是整个握手阶段的第三个随机数，这样服务器和客户端就同时有三个随机数，接着就用双方协

#### 商的加密算法， 各自生成 本次通信的「会话秘钥」。

#### 4. 服务器的最后回应

服务器收到客户端的第三个随机数（钥」。然后，向客户端发生最后的信息：pre-master key）之后，通过协商的加密算法，计算出本次通信的「会话秘

（ 1 ）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
（要，用来供客户端校验。 2 ）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘

至此，整个协议，只不过用「会话秘钥」加密内容。 SSL/TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP

## HTTP/1.1、HTTP/2、HTTP/3 演变^

#### 说说 HTTP/1.1 相比 HTTP/1.0 提高了什么性能？

#### HTTP/1.1 相比 HTTP/1.0 性能上的改进：

#### 使用 TCP ⻓连接的方式改善了 HTTP/1.0 短连接造成的性能开销。

```
支持管道（减少整体的响应时间。pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以
```

但 HTTP/1.1 还是有性能瓶颈：
请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分；
发送冗⻓的首部。每次互相发送相同的首部造成的浪费较多；
服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；
没有请求优先级控制；
请求只能从客户端开始，服务器只能被动响应。
那上面的 HTTP/1.1 的性能瓶颈，HTTP/2 做了什么优化？

HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。
那 HTTP/2 相比 HTTP/1.1 性能上的改进：

_1._ 头部压缩


HTTP/2 **复的部分** 会。 **压缩头** （Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你 **消除重**

这就是所谓的 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索
引号，以后就不发送同样字段了，只发送索引号，这样就 **提高速度** 了。

_2._ 二进制格式
HTTP/2 且统称为帧（不再像frame HTTP/1.1 ）： **头信息帧和数据帧** 里的纯文本形式的报文，而是全面采用了。 **二进制格式** ，头信息和数据体都是二进制，并

#### 这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这 增加了数据传输的效率 。

#### 3. 数据流

#### HTTP/2 包做标记，指出它属于哪个回应。的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据

每个请求或回应的所有数据包，称为一个数据流（定客户端发出的数据流编号为奇数， 服务器发出的数据流编号为偶数Stream ）。每个数据流都标记着一个独一无二的编号，其中规

客户端还可以 **指定数据流的优先级** 。优先级高的请求，服务器就先响应该请求。


#### 4. 多路复用

#### HTTP/2 是可以在 一个连接中并发多个请求或回应，而不用按照顺序一一对应 。

#### 移除了 了连接的利用率 HTTP/1.1 。中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题， 降低了延迟，大幅度提高

#### 举例来说，在一个回应 A 请求已经处理好的部分，接着回应 TCP 连接里，服务器收到了客户端 B 请求，完成后，再回应 A 和 B 的两个请求，如果发现 A 请求剩下的部分。 A 处理过程非常耗时，于是就


#### 5. 服务器推送

#### HTTP/2 送消息。还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以 主动 向客户端发

举例来说，在浏览器刚请求 **少延时的等待** ，也就是服务器推送（ HTML 的时候，就提前把可能会用到的Server Push，也叫 Cache Push JS）。、CSS 文件等静态资源主动发给客户端， **减**

```
HTTP/2 有哪些缺陷？HTTP/3 做了哪些优化？
```

HTTP/2 的。所以一旦发生了丢包现象，就会触发主要的问题在于，多个 HTTP 请求在复用一个 TCP 的重传机制，这样在一个 TCP 连接，下层的 TCP TCP 连接中的协议是不知道有多少个 **所有的 HTTP 请求都必须等** HTTP 请求
**待这个丢了的包被重传回来** 。
HTTP/1.1 中的管道（ pipeline）传输中如果有一个请求阻塞了，那么队列后请求也统统被阻塞住了
HTTP/2 多个请求复用一个TCP连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。
这都是基于 TCP 传输层的问题，所以 **HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！**


#### UDP 题。发生是不管顺序，也不管丢包的，所以不会出现 HTTP/1.1 的队头阻塞 和 HTTP/2 的一个丢包全部重传问

#### 大家都知道 UDP 是不可靠传输的，但基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输。

#### QUIC 影响 。有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流， 其他流不会受到

```
TLS3 升级成了最新的 1.3 版本，头部压缩算法也升级成了 QPack。
```

HTTPS 把以往的要建立一个连接，要花费 TCP 和 TLS/1.3 的 6 6 次交互次交互，先是建立三次握手，然后是 **合并成了 3 次，减少了交互次数** (^) 。TLS/1.3 的三次握手。QUIC 直接


#### 所以， QUIC 是一个在 UDP 之上的 伪 TCP + TLS + HTTP/2 的多路复用的协议。

#### QUIC HTTP/3 是新协议，对于很多网络设备，根本不知道什么是现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 QUIC，只会当做 TCP UDP。 ，这样会出现新的问题。所以

#### 参考资料：

#### [1] 上野 宣.图解HTTP.人⺠邮电出版社.

#### [2] 罗剑锋.透视HTTP协议.极客时间.

[3] 陈皓.HTTP的前世今.酷壳CoolShell.https://coolshell.cn/articles/19840.html
[4] 阮一峰.HTTP 协议入⻔.阮一峰的网络日志.http://www.ruanyifeng.com/blog/2016/08/http.html

## 读者问答^

```
读者问：“https和http相比，就是传输的内容多了对称加密，可以这么理解吗？”
```

1. 建立连接时候：https 比 http多了 TLS 的握手过程；
2. 传输内容的时候：https 会把数据进行加密，通常是对称加密数据；
    读者问：“ 我看文中 TLS 和 SSL 没有做区分，这两个需要区分吗？”

这两实际上是一个东⻄。
SSL 设计的。是洋文 “ _Secure Sockets Layer_ 的缩写，中文叫做「安全套接层」。它是在上世纪 90 年代中期，由网景公司

到了称改为 (^1999) TLS年，（是SSL “ _Transport Layer Security_ 因为应用广泛，已经成为互联网上的事实标准。” 的缩写），中文叫做 「传输层安全协议」。IETF 就在那年把 SSL 标准化。标准化之后的名
很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东⻄的不同阶段。
读者问：“为啥 ssl 的握手是 4 次？”
SSL/TLS 1.2 送，就是 4 次握手：需要 4 握手，需要 2 个 RTT 的时延，我文中的图是把每个交互分开画了，实际上把他们合在一起发

#### 另外， SSL/TLS 1.3 优化了过程，只需要 1 个 RTT 往返时延，也就是只需要 3 次握手：


## 最后^

#### 本文的 30 张图片，都是从一条线两条线画出来，灰常的费劲，深切感受到画图也是个 体力活 啊！

#### 爱偷懒的我其实不爱画图，但为了让大家能更好的理解，在跟自己无数次斗争后，踏上了耗时耗体力的画图的不归路，希望对你们有帮助！

**小林是专为大家图解的工具人，Goodbye，我们下次⻅！**


## 2.2 HTTP/1.1如何优化？

#### 问你一句：「 你知道 HTTP/1.1 该如何优化吗？ 」

我想你第一时间想到的是，使用 KeepAlive 将 HTTP/1.1 从短连接改成⻓链接。
这个确实是一个优化的手段，它是从底层的传输层这一方向入手的，通过减少少了网络传输的延迟，从而提高 HTTP/1.1 协议的传输效率。 TCP 连接建立和断开的次数，来减

但其实还可以从其他方向来优化 HTTP/1.1 协议，比如有如下 3 种优化思路：
尽量避免发送 _HTTP_ 请求；
在需要发送 _HTTP_ 请求时，考虑如何减少请求次数；
减少服务器的 _HTTP_ 响应的数据大小；
下面，就针对这三种思路具体看看有哪些优化方法。

## 如何避免发送 HTTP 请求？^


#### 这个思路你看到是不是觉得很奇怪，不发送氓嘛？ HTTP 请求，那还客户端还怎么和服务器交互数据？小林你这不是耍流

#### 冷静冷静，你说的没错，客户端当然要向服务器发送请求的。

#### 但是，对于一些具有重复性的数据都 缓存在本地 ，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求 HTTP/1.1-响应」的

#### 的性能肯定肉眼可⻅的提升。

#### 所以，避免发送头部有不少是针对缓存的字段。 HTTP 请求的方法就是通过 缓存技术 ，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的

#### 那缓存是如何做到的呢？

客户端会把第一次请求以及响应的数据保存在本地磁盘上，其中将请求的形成映射关系。 URL 作为 key，而响应作为 value，两者

这样当后续发起相同的请求时，就可以先在本地磁盘上通过直接从本地读取该响应。毋庸置疑，读取本次磁盘的速度肯定比网络请求快得多，如下图： key 查到对应的 value，也就是响应，如果找到了，就


#### 聪明的你可能想到了，万一缓存的响应不是最新的，而客户端并不知情，那么该怎么办呢？

#### 放心，这个问题 HTTP 设计者早已考虑到。

#### 所以，服务器在发送应头部的信息时，一旦发现缓存的响应是过期的，则就会重新发送网络请求。 HTTP 响应时，会估算一个过期的时间，并把这个信息放到响应头部中，这样客户端在查看响HTTP 关于缓说明会的头部字段很

#### 多，这部分内容留在下次文章，这次暂时不具体说明。


#### 如果客户端从第一次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，假设服务器上的资源并没有变更，还是老样子，那么你觉得还要在服务器的响应带上这个资源吗？

#### 很显然不带的话，可以提高 HTTP 协议的性能，那具体如何做到呢？

只需要客户端在重新发送请求时，在请求的识响应的资源，当服务器收到请求后，会将本地资源的摘要与请求中的摘要做个比较。 Etag 头部带上第一次请求的响应头部中的摘要，这个摘要是唯一标

如果不同，那么说明客户端的缓存已经没有价值，服务器在响应中带上最新的资源。
如果相同，说明客户端的缓存还是可以继续使用的，那么服务器告诉客户端仍然有效，这样就可以减少响应资源在网络中传输的延时，如下图： **仅返回不含有包体的 304 Not Modified 响应** ，


缓存真的是性能优化的一把万能钥匙，小到 CPU Cache、Page Cache、Redis Cache，大到 HTTP 协议的缓存。

## 如何减少 HTTP 请求次数？^

#### 减少 HTTP 请求次数自然也就提升了 HTTP 性能，可以从这 3 个方面入手：

#### 减少重定向请求次数；


#### 合并请求；

#### 延迟发送请求；

### 减少重定向请求次数

#### 我们先来看看什么是 重定向请求 ？

服务器上的一个资源可能由于迁移、维护等原因从时服务器不能粗暴地返回错误，而是通过 302 响应码和 url1 移至 Location url2 后，而客户端不知情，它还是继续请求 头部，告诉客户端该资源已经迁移至 url1 url2，这
了，于是客户端需要再发送 url2 请求以获得服务器的资源。
那么，如果重定向请求越多，那么客户端就要多次发起越降低网络性能。 HTTP 请求，每一次的 HTTP 请求都得经过网络，这无疑会

另外，服务端这一方往往不只有一台服务器，比如源服务器上一级是代理服务器，然后代理服务器才与客户端通信，这时客户端重定向就会导致客户端与代理服务器之间需要 2 次消息传递，如下图：

#### 如果 重定向的工作交由代理服务器完成，就能减少 HTTP 请求次数了 ，如下图：


#### 而且当代理服务器知晓了重定向规则后，可以进一步减少消息传递次数，如下图：


#### 除了 302 重定向响应码，还有其他一些重定向的响应码，你可以从下图看到：

其中，访问服务器的资源。 301 和 308 响应码是告诉客户端可以将重定向响应缓存到本地磁盘，之后客户端就自动用 url2 替代 url1


### 合并请求

#### 如果把多个访问小文件的请求合并成一个大的请求，虽然传输的总资源还是一样，但是减少请求，也就意味着 减少

#### 了重复发送的 HTTP 头部 。

#### 另外由于于是为了防止单个请求的阻塞，所以一般浏览器会同时发起 HTTP/1.1 是请求响应模型，如果第一个发送的请求，未收到对应的响应，那么后续的请求就不会发送， 5-6 个请求，每一个请求都是不同的 TCP 连接，那么

#### 如果合并了请求，也就会 减少 TCP 连接的数量，因而省去了 TCP 握手和慢启动过程耗费的时间 。

#### 接下来，具体看看合并请求的几种方式。

有的网⻚会含有很多小图片、小图标，有多少个小图片，客户端就要发起多少次请求。那么对于这些小图片，我们可以考虑使用 CSS Image Sprites 技术把它们合成一个大图片，这样浏览器就可以用一次请求获得一个大图片，
然后再根据 CSS 数据把大图片切割成多张小图片。

#### 这种方式就是 少网络的开销通过将多个小图片合并成一个大图片来减少 。 HTTP 请求的次数，以减少 HTTP 请求的次数，从而减

除了将小图片合并成大图片的方式，还有服务端使用件，也是能达到类似的效果。 webpack 等打包工具将 js、css 等资源合并打包成大文

另外，还可以将图片的二进制数据用发送. base64 编码后，以 URL 的形式潜入到 HTML 文件，跟随 HTML 文件一并

<imagesrc="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPoAAAFKCAIAAAC7M9WrAAAACXBIWXMAA ... (^)
/>


#### 这样客户端收到少了请求的次数。 HTML 后，就可以直接解码出数据，然后直接显示图片，就不用再发起图片相关的请求，这样便减

#### 可以看到， 合并请求的方式就是合并资源，以一个大资源的请求替换多个小资源的请求 。

#### 但是这样的合并请求会带来新的问题， 资源文件 ，这显然带来了额外的网络消耗。 当大资源中的某一个小资源发生变化后，客户端必须重新下载整个完整的大

### 延迟发送请求

#### 不要一口气吃成大胖子，一般于是可以通过「 按需获取 」的方式，来减少第一时间的 HTML 里会含有很多 HTTP HTTP 的 URL请求次数。，当前不需要的资源，我们没必要也获取过来，

#### 请求网⻚的时候，没必要把全部资源都获取到，而是只获取当前用户所看到的⻚面资源，当用户向下滑动⻚面的时候，再向服务器获取接下来的资源，这样就达到了延迟发送请求的效果。

## 如何减少 HTTP 响应的数据大小？^

#### 对于 HTTP 的请求和响应，通常 HTTP 的响应的数据大小会比较大，也就是服务器返回的资源会比较大。

#### 于是，我们可以考虑对响应的资源进行 压缩 ，这样就可以减少响应的数据大小，从而提高网络传输的效率。

#### 压缩的方式一般分为 2 种，分别是：


#### 无损压缩；

#### 有损压缩；

### 无损压缩

#### 无损压缩是指资源经过压缩后，信息不被破坏，还能完全恢复到压缩前的原样，适合用在文本文件、程序可执行文件、程序源代码。

#### 首先，我们针对代码的语法规则进行压缩，因为通常代码文件都有很多换行符或者空格，这些是为了帮助程序员更好的阅读，但是机器执行时并不要这些符，把这些多余的符号给去除掉。

#### 接下来，就是无损压缩了，需要对原始资源建立统计模型，利用这个统计模型，将常出现的数据用较短的二进制比特序列表示，将不常出现的数据用较⻓的二进制比特序列表示，生成二进制比特序列一般是「霍夫曼编码」算法。

gzip 段告诉服务器：就是比较常⻅的无损压缩。客户端支持的压缩算法，会在 HTTP 请求中通过头部中的 Accept-Encoding 字

服务器收到后，会从中选择一个服务器支持的或者合适的压缩算法，然后使用此压缩算法对响应资源进行压缩，最后通过响应头部中的 content-encoding 字段告诉客户端该资源使用的压缩算法。

gzip 择压缩效率更高的的压缩效率相比 br Google 压缩算法。推出的 Brotli 算法还是差点意思，也就是上文中的 br，所以如果可以，服务器应该选

### 有损压缩

#### 与无损压缩相对的就是有损压缩，经过此方法压缩，解压的数据会与原始数据不同但是非常接近。

#### 有损压缩主要将次要的数据舍弃，牺牲一些质量来减少数据量、提高压缩比，这种方法经常用于压缩多媒体数据，比如音频、视频、图片。

可以通过 HTTP 请求头部中的 Accept 字段里的「 q 质量因子」，告诉服务器期望的资源质量。

关于图片的压缩，目前压缩比较高的是下图： Google 推出的 **WebP 格式** ，它与常⻅的 Png 格式图片的压缩比例对比如

```
Accept-Encoding: gzip, deflate, br
```

```
content-encoding: gzip
```

```
Accept: audio/*; q=0.2, audio/basic
```

可以发现，相同图片质量下，使用 WebP 格式的图片，这将大幅度提升网络传输的性能。WebP 格式的图片大小都比 Png 格式的图片小，所以对于大量图片的网站，可以考虑

关于音视频的压缩，音视频主要是动态的，每个帧都有时序的关系，通常时间连续的帧之间的变化是很小的。
比如，一个在看书的视频，画面通常只有人物的手和书桌上的书是会有变化的，而其他地方通常都是静态的，于是只需要在一个静态的关键帧，使用 **增量数据** 来表达后续的帧，这样便减少了很多数据，提高了网络传输的性能。对
于视频常⻅的编码格式有 H264、H265 等，音频常⻅的编码格式有 AAC、AC3。

## 总结^

#### 这次主要从 3 个方面介绍了优化 HTTP/1.1 协议的思路。

#### 第一个思路是，通过缓存技术来避免发送盘，下次请求的时候，如果缓存没过期，就直接读取本地缓存的响应数据。如果缓存过期，客户端发送请求的时候 HTTP 请求。客户端收到第一个请求的响应后，可以将其缓存在本地磁

#### 带上响应数据的摘要，服务器比对后发现资源没有变化，就发出不带包体的有效。 304 响应，告诉客户端缓存的响应仍然

#### 第二个思路是，减少 HTTP 请求的次数，有以下的方法：

#### 1. 将原本由客户端处理的重定向请求，交给代理服务器处理，这样可以减少重定向请求的次数；

#### 2. 将多个小资源合并成一个大资源再传输，能够减少接数量，进而省去 TCP 握手和慢启动的网络消耗； HTTP 请求次数以及 头部的重复传输，再来减少 TCP 连

#### 3. 按需访问资源，只访问当前用户看得到迟请求，也就减少了同一时间的 HTTP /请求次数。用得到的资源，当客户往下滑动，再访问接下来的资源，以此达到延

#### 第三思路是，通过压缩响应资源，降低传输资源的大小，从而提高传输效率，所以应当选择更优秀的压缩算法。


#### 不管怎么优化和 HTTP/3 协议。 HTTP/1.1 协议都是有限的，不然也不会出现 HTTP/2 和 HTTP/3 协议，后续我们再来介绍 HTTP/2

#### 好了，此次分享到这就结束了，如果这篇文章对你有帮助，欢迎来个三连，你们的支持就是小林的最大动力，我们下次⻅！

#### 参考资料：

1. https://isparta.github.io/compare-webp/index.html
2. https://zh.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Lossy_compression
3. https://en.wikipedia.org/wiki/Lossless_compression
4. https://time.geekbang.org/column/article/242667
5. https://www.tutorialrepublic.com/css-tutorial/css-sprites.php
6. https://blog.csdn.net/weixin_38055381/article/details/81504716
7. https://blog.csdn.net/weixin_44151887/article/details/106278559

## 最后^

#### 哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！


## 2.3 HTTPS RSA 握手解析

#### 我很早之前写过一篇关于以这次我们再来深入一下 HTTP HTTPS和，用 HTTPS 实战抓包 的文章，但对于的方式，带大家再来窥探一次 HTTPS 介绍还不够详细，只讲了比较基础的部分，所 HTTPS。

#### 对于还不知道对称加密和非对称加密的同学，你先复习我以前的这篇文章题」，本篇文章默认大家已经具备了这些知识。 「硬核！ 30 张图解 HTTP 常⻅的面试

## TLS 握手过程^

#### HTTP 都可以截获通信的内容。由于是明文传输，所谓的明文，就是说客户端与服务端通信的信息都是肉眼可⻅的，随意使用一个抓包工具

#### 所以安全上存在以下三个⻛险：

#### 窃听⻛险，比如通信链路上可以获取通信内容，用户号容易没。

#### 篡改⻛险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。

#### 冒充⻛险，比如冒充淘宝网站，用户钱容易没。

#### HTTP S 在 HTTP 与 TCP 层之间加入了 TLS 协议，来解决上述的⻛险。


#### TLS 协议是如何解决 HTTP 的⻛险的呢？

#### 信息加密： HTTP 交互信息是被加密的，第三方就无法被窃取；

#### 校验机制：校验信息传输过程中是否有被第三方篡改过，如果被篡改过，则会有警告提示；

#### 身份证书：证明淘宝是真的淘宝网；

#### 可⻅，有了握手过程，如下图： TLS 协议，能保证 HTTP 通信是安全的了，那么在进行 HTTP 通信前，需要先进行 TLS 握手。TLS 的


上图简要概述来位，类似于 TCP TLS 里的的握手过程，其中每一个「框」都是一个记录（ segment。多个记录可以组合成一个 TCP 包发送，所以 _record_ ），记录是 **通常经过「四个消息」就可以完成** TLS 收发数据的基本单
**TLS** 议。 **握手，也就是需要 2 个 RTT 的时延** ，然后就可以在安全的通信环境里发送 HTTP 报文，实现 HTTPS 协

所以可以发现，连接。 HTTPS 是应用层协议，需要先完成 TCP 连接建立，然后走 TLS 握手过程后，才能建立通信安全的


#### 事实上，不同的密钥交换算法，TLS 的握手过程可能会有一些区别。

#### 这里先简单介绍下密钥交换算法，因为考虑到性能的问题，所以双方在加密应用信息时使用的是对称加密密钥，而对称加密密钥是不能被泄漏的，为了保证对称加密密钥的安全性，所以使用非对称加密的方式来保护对称加密密钥

#### 的协商，这个工作就是密钥交换算法负责的。

#### 接下来，我们就以最简单的 RSA 密钥交换算法，来看看它的 TLS 握手过程。

## RSA 握手过程^

#### 传统的私钥，其中公钥会在 TLS 握手基本都是使用 TLS 握手阶段传递给客户端，私钥则一直留在服务端，一定要确保私钥不能被窃取。 RSA 算法来实现密钥交换的，在将 TLS 证书部署服务端时，证书文件中包含一对公

#### 在法，公钥加密的消息仅能通过私钥解密，这样服务端解密后，双方就得到了相同的密钥，再用它加密应用消息。 RSA 密钥协商算法中，客户端会生成随机密钥，并使用服务端的公钥加密后再传给服务端。根据非对称加密算

我用 Wireshark 工具抓了用 RSA 密钥交换的 TLS 握手过程，你可以从下面看到，一共经历来四次握手：

对应 Wireshark 的抓包，我也画了一幅图，你可以从下图很清晰地看到该过程：


#### 那么，接下来针对每一个 TLS 握手做进一步的介绍。


### TLS 第一次握手

客户端首先会发一个「 **Client Hello** 」消息，字面意思我们也能理解到，这是跟服务器「打招呼」。

消息里面有客户端使用的数会被服务端保留，它是生成对称加密密钥的材料之一。 TLS 版本号、支持的密码套件列表，以及生成的 **随机数（** **_Client Random_** **）** ，这个随机

### TLS 第二次握手

当服务端收到客户端的「套件，以及生成 **随机数（** Client Hello **_Server Random_** 」消息后，会确认 **）** 。 TLS 版本号是否支持，和从密码套件列表中选择一个密码

接着，返回「然后从客户端的密码套件列表选择了一个合适的密码套件。 **Server Hello** 」消息，消息里面有服务器确认的 TLS 版本号，也给出了随机数（Server Random），


可以看到，服务端选择的密码套件是 “Cipher Suite: TLS_RSA_WITH_AES_128_GCM_SHA256”。
这个密码套件看起来真让人头晕，好一大串，但是其实它是有固定格式和规范的。基本的形式是「 **签名算法 + 对称加密算法 + 摘要算法** 」， 一般 WITH 单词前面有两个单词，第一个单词是约定密钥交换的算法， **密钥交换算法 +**
第二个单词是约定证书的验证算法。比如刚才的密码套件的意思就是：
由于 WITH 单词只有一个 RSA，则说明握手时密钥交换算法和签名算法都是使用 RSA；
握手后的通信使用 AES 对称算法，密钥⻓度 128 位，分组模式是 GCM；
摘要算法 SHA384 用于消息认证和产生随机数；
就前面这两个客户端和服务端相互「打招呼」的过程，客户端和服务端就已确认了而且你可能发现客户端和服务端都会各自生成一个随机数，并且还会把随机数传递给对方。 TLS 版本和使用的密码套件，

那这个随机数有啥用呢？其实这两个随机数是后续作为生成「会话密钥」的条件，所谓的会话密钥就是数据传输时，所使用的对称加密密钥。

然后，服务端为了证明自己的身份，会发送「 **Server Certificate** 」给客户端，这个消息里含有数字证书。

随后，服务端发了「呼完毕。 **Server Hello Done** 」消息，目的是告诉客户端，我已经把该给你的东⻄都给你了，本次打招


### 客户端验证证书

#### 在这里刹个⻋，客户端拿到了服务端的数字证书后，要怎么校验该数字证书是真实有效的呢？

#### 数字证书和 CA 机构

#### 在说校验数字证书是否可信的过程前，我们先来看看数字证书是什么，一个数字证书通常包含了：

#### 公钥；

#### 持有者信息；

#### 证书认证机构（CA）的信息；

#### CA 对这份文件的数字签名及使用的算法；

#### 证书有效期；

#### 还有一些其他额外信息；

#### 那数字证书的作用，是用来认证公钥持有者的身份，以防止第三方进行冒充。说简单些，证书就是用来告诉客户端，该服务端是否是合法的，因为只有证书合法，才代表服务端身份是可信的。

#### 我们用证书来认证公钥持有者的身份（服务端的身份），那证书又是怎么来的？又该怎么认证证书呢？

为了让服务端的公钥被大家信任，服务端的证书都是由就是网络世界里的公安局、公证中心，具有极高的可信度，所以由它来给各个公钥签名，信任的一方签发的证书， CA （ _Certificate Authority_ ，证书认证机构）签名的，CA
那必然证书也是被信任的。
之所以要签名，是因为签名的作用可以避免中间人在获取证书时对证书内容的篡改。
**数字证书签发和验证流程**
如下图图所示，为数字证书签发和验证流程：


#### CA 签发证书的过程，如上图左边部分：

首先得到一个 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 值； Hash 计算，
然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；
最后将 Certificate Signature 添加在文件证书上，形成数字证书；
客户端校验服务端的数字证书的过程，如上图右边部分：
首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1；
通常浏览器和操作系统中集成了Signature 内容，得到一个 Hash CA 值 H2 的公钥信息，浏览器收到证书后可以使用； CA 的公钥解密 Certificate
最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。
**证书链**
但事实上，证书的验证过程中还存在一个证书信任链的问题，因为我们向而是由中间证书签发的，比如百度的证书，从下图你可以看到，证书的层级有三级： CA 申请的证书一般不是根证书签发的，


#### 对于这种三级层级关系的证书的验证过程如下：

客户端收到钥去验证 baidu.com baidu.com 证书是否可信。于是，客户端根据的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公 baidu.com 证书中的签发者，找到该证书的颁发机构
是 “GlobalSign Organization Validation CA - SHA256 - G2”，然后向 CA 请求该中间证书。
请求到证书后发现签发的，由于 “GlobalSign Root CA” “GlobalSign Organization Validation CA - SHA256 - G2” 没有再上级签发机构，说明它是根证书，也就是自签证书。应用软件会证书是由 “GlobalSign Root CA”
检查此证书有否已预载于根证书清单上，如果有，则可以利用根证书中的公钥去验证Organization Validation CA - SHA256 - G2” 证书，如果发现验证通过，就认为该中间证书是可信的。 “GlobalSign
“GlobalSign Organization Validation CA - SHA256 - G2” Validation CA - SHA256 - G2” 证书中的公钥去验证 baidu.com 证书被信任后，可以使用证书的可信性，如果验证通过，就可以信任 “GlobalSign Organization
baidu.com 证书。
在这四个步骤中，最开始客户端只信任根证书“GlobalSign Organization Validation CA - SHA256 - G2” GlobalSign Root CA 证书，而 “GlobalSign Organization Validation CA -证书的，然后 “GlobalSign Root CA” 证书信任
SHA256 - G2” 证书又信任 baidu.com 证书，于是客户端也信任 baidu.com 证书。
总括来说，由于用户信任作系统或浏览器的软件商，所以由软件商预载了根证书的 GlobalSign，所以由 GlobalSign GlobalSign 所担保的 baidu.com 都可被信任。可以被信任，另外由于用户信任操


#### 操作系统里一般都会内置一些根证书，比如我的 MAC 电脑里内置的根证书有这么多：


#### 这样的一层层地验证就构成了一条信任链路，整个证书信任链验证流程如下图所示：

最后一个问题，为什么需要证书链这么麻烦的流程？呢？ Root CA 为什么不直接颁发证书，而是要搞那么多中间层级

这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。

### TLS 第三次握手

客户端验证完证书后，认为可信则继续往下走。接着，客户端就会生成一个新的 **随机数 (** **_pre-master_** **)** ，用服务器
的 RSA 公钥加密该随机数，通过「 **Change Cipher Key Exchange** 」消息传给服务端。


服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数 (pre-master)。
至此， **客户端和服务端双方都共享了三个随机数，分别是 Client Random、Server Random、pre-master** 。
于是，双方根据已经得到的三个随机数，生成请求/响应的数据加解密。 **会话密钥（Master Secret）** ，它是对称密钥，用于对后续的 HTTP

生成完会话密钥后，然后客户端发一个「 **Change Cipher Spec** 」，告诉服务端开始使用加密方式发送消息。

然后，客户端再发一个「要，再用会话密钥（master secret **Encrypted Handshake Message** ）加密一下，让服务器做个验证，验证加密通信是否可用和之前握手信息是否有 **（Finishd）** 」消息，把之前所有发送的数据做个摘
被中途篡改过。

可以发现，「Change Cipher Spec」之前传输的 TLS 握手数据都是明文，之后都是对称密钥加密的密文。

### TLS 第四次握手

服务器也是同样的操作，发「验证加密和解密没问题，那么握手正式完成。 **Change Cipher Spec** 」和「 **Encrypted Handshake Message** 」消息，如果双方都

最后，就用「会话密钥」加解密 HTTP 请求和响应了。

## RSA 算法的缺陷^


#### 使用 一）给服务端时使用的是公钥加密的，服务端收到到后，会用私钥解密得到随机数。所以一旦服务端的私钥泄漏 RSA 密钥协商算法的最大问题是不支持前向保密 。因为客户端传递随机数（用于生成对称加密密钥的条件之

#### 了，过去被第三方截获的所有 TLS 通讯密文都会被破解。

#### 为了解决这一问题，于是就有了 DH 密钥协商算法，这里简单介绍它的工作流程。

#### 客户端和服务端各自会生成随机数，并以此作为私钥，然后根据公开的握手双方交换各自的公钥，这样双方都有自己的私钥和对方的公钥，然后双方根据各自持有的材料算出一个随机 DH 计算公示算出各自的公钥，通过 TLS

#### 数，这个随机数的值双方都是一样的，这就可以作为后续对称加密时使用的密钥。

#### DH 密钥的，而且每一次对称加密密钥都是实时生成的，实现前向保密 密钥交换过程中， 即使第三方截获了 TLS 握手阶段传递的公钥，在不知道的私钥的情况下，也是无法计算出 。

#### 但因为钥协商算法，关于 DH 算法的计算效率问题，后面出现了 ECDHE 握手的过程，将在下一篇揭晓，尽情期待哦。 ECDHE 密钥协商算法，我们现在大多数网站使用的正是 ECDHE 密

## 最后^


#### 哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！

## 2.4 HTTPS ECDHE 握手解析

#### HTTPS 常用的密钥交换算法有两种，分别是 RSA 和 ECDHE 算法。

#### 其中，具有前向安全，所以被广泛使用。RSA 是比较传统的密钥交换算法，它不具备前向安全的性质，因此现在很少服务器使用的。而 ECDHE 算法

#### 我在上一篇已经介绍了 RSA 握手的过程，今天这一篇就「从理论再到实战抓包」介绍 ECDHE 算法 。


## 离散对数^

#### ECDHE 密钥协商算法是 DH 算法演进过来的，所以我们先从 DH 算法说起。

#### DH 算法是非对称加密算法， 因此它可以用于密钥交换，该算法的核心数学思想是 离散对数 。


#### 是不是听到这个数学概念就怂了？不怕，这次不会说离散对数推到的过程，只简单提一下它的数学公式。

#### 离散对数是「离散 + 对数」的两个数学概念的组合，所以我们先来复习一遍对数。

#### 要说起对数，必然要说指数，因为它们是互为反函数，指数就是幂运算，对数是指数的逆运算。

#### 举个栗子，如果以 2 作为底数，那么指数和对数运算公式，如下图所示：

#### 那么对于底数为 2 的时候， 32 的对数是 5 ， 64 的对数是 6 ，计算过程如下：

#### 对数运算的取值是可以连续的，而离散对数的取值是不能连续的，因此也以「离散」得名，

离散对数是在对数运算的基础上加了「模运算」，也就说取余数，对应编程语言的操作符是「表示。离散对数的概念如下图： %」，也可以用 mod


上图的，底数上面的公式计算出真数。但反过来，知道真数却很难推算出对数。 a 和模数 p 是离散对数的公共参数，也就说是公开的，b 是真数，i 是对数。知道了对数，就可以用

**特别是当模数 p 是一个很大的质数，即使知道底数 a 和真数 b ，在现有的计算机的计算水平是几乎无法算出离散
对数的，这就是 DH 算法的数学基础。**

## DH 算法^

#### 认识了离散对数，我们来看看 DH 算法是如何密钥交换的。

#### 现假设小红和小明约定使用的参数，这两个参数是公开的，用 DH 算法来交换密钥，那么基于离散对数，小红和小明需要先确定模数和底数作为算法 P 和 G 来代称。

然后小红和小明各自生成一个随机整数作为小明的私钥用 b 代称。 **私钥** ，双方的私钥要各自严格保管，不能泄漏，小红的私钥用 a 代称，

现在小红和小明双方都有了 P 和 G 以及各自的私钥，于是就可以计算出 **公钥** ：
小红的公钥记作 A，A = G ^ a ( mod P )；
小明的公钥记作 B，B = G ^ b ( mod P )；
A 计算机的计算能力是无法破解的，如果量子计算机出来了，那就有可能被破解，当然如果量子计算机真的出来了，和 B 也是公开的，因为根据离散对数的原理，从真数（A 和 B）反向计算对数 a 和 b 是非常困难的，至少在现有
那么密钥协商算法就要做大的升级了。
双方交换各自A。 DH 公钥后，小红手上共有 5 个数：P、G、a、A、B，小明手上也同样共有 5 个数：P、G、b、B、

然后小红执行运算：mod P )，得到的结果也是 B ^ a ( mod P ) K。 ，其结果为 K，因为离散对数的幂运算有交换律，所以小明执行运算： A ^ b (


#### 这个 K 就是小红和小明之间用的 对称加密密钥 ，可以作为会话密钥使用。

可以看到，整个密钥协商过程中，小红和小明公开了是公钥，而 a、b 是双方各自保管的私钥，黑客无法获取这 4 个信息： 2 个私钥，因此黑客只能从公开的P、G、A、B，其中 P、G 是算法的参数， P、G、A、B A 入手，和 B
计算出离散对数（私钥）。
前面也多次强调，的，破解不出私钥，也就无法计算出会话密钥，因此 根据离散对数的原理，如果 P 是一个大数，在现有的计算机的计算能力是很难破解出 DH 密钥交换是安全的。 私钥 a、b

## DHE 算法^

#### 根据私钥生成的方式，DH 算法分为两种实现：

static DH 算法，这个是已经被废弃了；
DHE 算法，现在常用的；
static DH 固定，即 a 算法里有一方的私钥是静态的，也就说每次密钥协商的时候有一方的私钥都是一样的，一般是服务器方不变，客户端的私钥则是随机生成的。

于是，量的密钥协商过程的数据，因为密钥协商的过程有些数据是公开的，黑客就可以依据这些数据暴力破解出服务器的DH 交换密钥时就只有客户端的公钥是变化，而服务端公钥是不变的，那么随着时间延⻓，黑客就会截获海
私钥，然后就可以计算出会话密钥了，于是之前截获的加密数据会被破解，所以 **性** 。 **static DH 算法不具备前向安全**

既然固定一方的私钥有被破解的⻛险，那么干脆就让双方的私钥在每次密钥交换通信时，都是随机生成的、临时的，这个方式也就是 DHE 算法，E 全称是 ephemeral（临时性的）。


#### 所以，即使有个牛逼的黑客破解了某一次通信过程的私钥，其他通信过程的私钥仍然是安全的，因为 的私钥都是没有任何关系的，都是独立的，这样就保证了「前向安全」 。 每个通信过程

## ECDHE 算法^

#### DHE 钥交换算法算法由于计算性能不佳，因为需要做大量的乘法，为了提升 —— ECDHE 算法 。 DHE 算法的性能，所以就出现了现在广泛用于密

#### ECDHE 话密钥。算法是在 DHE 算法的基础上利用了 ECC 椭圆曲线特性，可以用更少的计算量计算出公钥，以及最终的会

#### 小红和小明使用 ECDHE 密钥交换算法的过程：

#### 双方事先确定好使用哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的；

双方各自随机生成一个随机数作为和 d1，小明的公私钥为 Q2 和 d2； **私钥d** ，并与基点 G相乘得到 **公钥Q** （Q = dG），此时小红的公私钥为 Q1
双方交换各自的公钥，最后小红计算点（上是可以满足乘法交换和结合律，所以 d1Q2 = d1d2G = d2d1G = d2Q1 x1，y1） = d1Q2，小明计算点（，因此x2，y2 **双方的** ） = d2Q1 **x 坐标是一样的，所** ，由于椭圆曲线
**以它是共享密钥，也就是会话密钥** 。
这个过程中，双方的私钥都是随机、临时生成的，都是不公开的，即使根据公开的信息（椭圆曲线、公钥、基点G）也是很难计算出椭圆曲线上的离散对数（私钥）。

## ECDHE 握手过程^

#### 知道了 ECDHE 算法基本原理后，我们就结合实际的情况来看看。

我用 Wireshark 工具抓了用 ECDHE 密钥协商算法的 TSL 握手过程，可以看到是四次握手：


#### 细心的小伙伴应该发现了，对于 RSA 握手过程，必须要完成 使用了 TLS ECDHE 四次握手，才能传输应用数据。 ，在 TLS 第四次握手前，客户端就已经发送了加密的 HTTP 数据 ，而

所以， _False Start_ **ECDHE** 」，跟「 **相比 RSA** _TCP Fast Open_ **握手过程省去了一个消息往返的时间** 」有点像，都是在还没连接完全建立前，就发送了应用数据，这样便提高了传，这个有点「抢跑」的意思，它被称为是「 _TLS_
输的效率。
接下来，分析每一个 ECDHE 握手过程。

### TLS 第一次握手

客户端首先会发一个「成的 **随机数（** **_Client Random_** **Client Hello）** 。」消息，消息里面有客户端使用的 TLS 版本号、支持的密码套件列表，以及生

### TLS 第二次握手

服务端收到客户端的「打招呼」，同样也要回礼，会返回「号，也给出了一个 **随机数（** **_Server Random_** **）** ，然后从客户端的密码套件列表选择了一个合适的密码套件。 **Server Hello** 」消息，消息面有服务器确认的 TLS 版本


#### 不过，这次选择的密码套件就和 RSA 不一样了，我们来分析一下这次的密码套件的意思。

#### 「 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384」

#### 密钥协商算法使用 ECDHE；

#### 签名算法使用 RSA；

#### 握手后的通信使用 AES 对称算法，密钥⻓度 256 位，分组模式是 GCM；

#### 摘要算法使用 SHA384；

接着，服务端为了证明自己的身份，发送「 **Certificate** 」消息，会把证书也发给客户端。

#### 这一步就和 RSA 握手过程有很大到区别了，因为服务端选择了 ECDHE 密钥协商算法，所以会在发送完证书后，

发送「 **Server Key Exchange** 」消息。


#### 这个过程服务器做了三件事：

选择了客户端； **名为 named_curve 的椭圆曲线** ，选好了椭圆曲线相当于椭圆曲线基点 G 也定好了，这些都会公开给
生成随机数作为服务端椭圆曲线的私钥，保留到本地；
根据基点 G 和私钥计算出 **服务端的椭圆曲线公钥** ，这个会公开给客户端。
为了保证这个椭圆曲线的公钥不被第三方篡改，服务端会用 RSA 签名算法给服务端的椭圆曲线公钥做个签名。
随后，就是「 **Server Hello Done** 」消息，服务端跟客户端表明：“这些就是我提供的信息，打招呼完毕”。

至此， **Random** TLS **、使用的椭圆曲线、椭圆曲线基点** 两次握手就已经完成了，目前客户端和服务端通过明文共享了这几个信息： **G、服务端椭圆曲线的公钥** ，这几个信息很重要，是后续生成会话密 **Client Random、Server**
钥的材料。

### TLS 第三次握手

#### 客户端收到了服务端的证书后，自然要校验证书是否合法，如果证书合法，那么服务端到身份就是没问题的。校验证书到过程，会走证书链逐级验证，确认证书的真实性，再用证书的公钥验证签名，这样就能确认服务端的身份

#### 了，确认无误后，就可以继续往下走。

客户端会生成一个随机数作为客户端椭圆曲线的私钥，然后再根据服务端前面给的信息，生成 **钥** ，然后用「 **Client Key Exchange** 」消息发给服务端。 **客户端的椭圆曲线公**


至此，双方都有对方的椭圆曲线公钥、自己的椭圆曲线私钥、椭圆曲线基点y），其中 x 坐标值双方都是一样的，前面说 ECDHE 算法时候，说 x 是会话密钥， G。于是，双方都就计算出点（ **但实际应用中，x 还不是最终** x，
**的会话密钥** 。
还记得 TLS 握手阶段，客户端和服务端都会生成了一个随机数传递给对方吗？
**最终的会话密钥，就是用「客户端随机数 + 服务端随机数 + x（ECDHE 算法算出的共享密钥） 」三个材料生成
的** 。
之所以这么麻烦，是因为把三个不可靠的随机数混合起来，那么「随机」的程度就非常高了，足够让黑客计算出最终的会话密钥，安全性更 TLS 设计者不信任客户端或服务器「伪随机数」的可靠性，为了保证真正的完全随机，
高。
算好会话密钥后，客户端会发一个「 **Change Cipher Spec** 」消息，告诉服务端后续改用对称算法加密通信。

接着，客户端会发「一下，让服务端做个验证，验证下本次生成的对称密钥是否可以正常使用。 **Encrypted Handshake Message** 」消息，把之前发送的数据做一个摘要，再用对称密钥加密

### TLS 第四次握手

最后，服务端也会有一个同样的操作，发「息，如果双方都验证加密和解密没问题，那么握手正式完成。于是，就可以正常收发加密的 **Change Cipher Spec** 」和「 **Encrypted Handshake Message** HTTP 请求和响应了。」消


## 总结^

#### RSA 和 ECDHE 握手过程的区别：

#### RSA 密钥协商算法「不支持」前向保密，ECDHE 密钥协商算法「支持」前向保密；

#### 使用了以不用等服务端的最后一次 RSA 密钥协商算法， TLS TLS 握手，就可以提前发出加密的完成四次握手后，才能进行应用数据传输，而对于 HTTP 数据，节省了一个消息的往返时间； ECDHE 算法，客户端可

```
使用过程没有该消息； ECDHE， 在 TLS 第 2 次握手中，会出现服务器端发出的「Server Key Exchange」消息，而 RSA 握手
```

参考资料：

1. https://zh.wikipedia.org/wiki/橢圓曲線迪菲-赫爾曼金鑰交換
2. https://zh.wikipedia.org/wiki/椭圆曲线
3. https://zh.wikipedia.org/wiki/迪菲-赫爾曼密鑰交換
4. https://time.geekbang.org/column/article/148188
5. https://zhuanlan.zhihu.com/p/106967180

## 最后^

#### 哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！


## 2.5 HTTPS 如何优化？

#### 由裸数据传输的也带来了性能消耗。 HTTP 协议转成加密数据传输的 HTTPS 协议，给应用数据套了个「保护伞」，提高安全性的同时

#### 因为 密钥 HTTPS ，这个过程最⻓可以花费掉相比 HTTP 协议多一个 2 RTT TLS ，接着后续传输的应用数据都得使用对称加密密钥来加密协议握手过程， 目的是为了通过非对称加密握手协商或者交换出对称加密 /解密。

#### 为了数据的安全性，我们不得不使用HTTPS 的优化是非常重要的。 HTTPS 协议，至今大部分网址都已从 HTTP 迁移至 HTTPS 协议，因此针对

#### 这次，就从多个⻆度来优化 HTTPS。


## 分析性能损耗^

#### 既然要对 HTTPS 优化，那得清楚哪些步骤会产生性能消耗，再对症下药。


#### 产生性能消耗的两个环节：

#### 第一个环节， TLS 协议握手过程；

#### 第二个环节，握手后的对称加密报文传输。

对于第二环节，现在主流的对称加密算法硬件级别的优化，因此这个环节的性能消耗可以说非常地小。 AES、ChaCha20 性能都是不错的，而且一些 CPU 厂商还针对它们做了

而第一个环节，会产生性能损耗，比如：TLS 协议握手过程不仅增加了网络延时（最⻓可以花费掉 2 RTT），而且握手过程中的一些步骤也

对于 ECDHE 密钥协商算法，握手过程中会客户端和服务端都需要临时生成椭圆曲线公私钥；
客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，目的是验证服务器的证书是否有被吊销；
双方计算 Pre-Master，也就是对称加密密钥；
为了大家更清楚这些步骤在 TLS 协议握手的哪一个阶段，我画出了这幅图：


## 硬件优化^

#### 玩游戏时，如果我们怎么都战胜不了对方，那么有一个最有效、最快的方式来变强，那就是「充钱」，如果还是不行，那说明你充的钱还不够多。


#### 对于计算机里也是一样，软件都是跑在物理硬件上，硬件越牛逼，软件跑的也越快，所以如果要优化化，最直接的方式就是花钱买性能参数更牛逼的硬件。 HTTPS 优

#### 但是花钱也要花对方向，应该花在 CPU 上。 HTTPS 协议是计算密集型，而不是 I/O 密集型 ，所以不能把钱花在网卡、硬盘等地方，

#### 一个好的TLS 握手过程。 CPU，可以提高计算性能，因为 HTTPS 连接过程中就有大量需要计算密钥的过程，所以这样可以加速

#### 另外，如果可以，应该选择可以法，这样便加速了数据的加解密传输过程。 支持 AES-NI 特性的 CPU ，因为这种款式的 CPU 能在指令级别优化了 AES 算

如果你的服务器是 Linux 系统，那么你可以使用下面这行命令查看 CPU 是否支持 AES-NI 指令集：


如果我们的加密算法，因为 CPU ChaCha20 支持 AES-NI 算法的运算指令相比特性，那么对于对称加密的算法应该选择 AES 算法会对 CPU 更友好一点。 AES 算法。否则可以选择 ChaCha20 对称

## 软件优化^

#### 如果公司预算充足对于新的服务器是可以考虑购买更好的可能就不太适合了，于是就要从软件的方向来优化了。 CPU，但是对于已经在使用的服务器，硬件优化的方式

#### 软件的优化方向可以分层两种，一个是 软件升级 ，一个是 协议优化 。

#### 先说第一个软件升级，软件升级就是将正在使用的软件升级到最新版本，因为最新版本不仅提供了最新的特性，也优化了以前软件的问题或性能。比如：

将 Linux 内核从 2.x 升级到 4.x；
将 OpenSSL 从 1.0.1 升级到 1.1.1；
...
看似简单的软件升级，对于有成百上千服务器的公司来说，软件升级也跟硬件升级同样是一个棘手的问题，因为要实行软件升级，会花费时间和人力，同时也存在一定的⻛险，也可能会影响正常的线上服务。

既然如此，我们把目光放到协议优化，也就是在现有的环节下，通过较小的改动，来进行优化。

## 协议优化^

#### 协议的优化就是对「密钥交换过程」进行优化。


### 密钥交换算法优化

#### TLS 1.2 传输，而且版本如果使用的是 RSA 密钥交换算法不具备前向安全性。 RSA 密钥交换算法，那么需要 4 次握手，也就是要花费 2 RTT，才可以进行应用数据的

#### 总之使用 RSA 密钥交换算法的 TLS 握手过程，不仅慢，而且安全性也不高 。

因此如果可以，尽量的意思，客户端可以在 **选用** TLS **ECDHE** 协议的第 **密钥交换** 3 次握手后，第算法替换 RSA 4 次握手前，发送加密的应用数据，以此将算法，因为该算法由于支持「False Start **TLS** 」，它是 **握手的消息** “抢跑”
**往返由 2 RTT 减少到 1 RTT，而且安全性也高，具备前向安全性** 。
ECDHE 快的椭圆曲线。算法是基于椭圆曲线实现的，不同的椭圆曲线性能也不同，应该尽量 **选择 x25519 曲线** ，该曲线是目前最

比如在 Nginx 上，可以使用 ssl_ecdh_curve 指令配置想使用的椭圆曲线，把优先使用的放在前面：

#### 对于对称加密算法方面，如果对安全性不是特别高的要求，可以些，因为密钥的⻓度短一些。 选用 AES_128_GCM ，它比 AES_256_GCM 快一

比如在且把性能最快最安全的算法放在最前面： Nginx 上，可以使用 ssl_ciphers 指令配置想使用的非对称加密算法和对称加密算法，也就是密钥套件，而


### TLS 升级

#### 当然，如果可以，直接把 RTT ，而且安全性更高。 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤， 完成 TLS 握手只要 1

在手） TLS 1.2 消息协商出后续使用的加密算法，再互相交换公钥（第的握手中，一般是需要 4 次握手，先要通过 Client Hello 3 和 第 4 （第次握手），然后计算出最终的会话密钥，下 1 次握手）和 Server Hello（第 2 次握
图的左边部分就是 TLS 1.2 的握手过程：

上图的右边部分就是 **于是这样就减少到只需** TLS 1.3 **1 RTT** 的握手过程，可以发现 **就能完成 TLS 握手** 。 **TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，**

怎么合并的呢？具体的做法是，客户端在钥。 Client Hello 消息里带上了支持的椭圆曲线，以及这些椭圆曲线对应的公

服务端收到后，选定一个椭圆曲线等参数，然后返回消息时，带上服务端这边的公钥。经过这已经有生成会话密钥的材料了，于是客户端计算出会话密钥，就可以进行应用数据的加密传输了。 1 个 RTT，双方手上

而且， **对于密钥交换算法，废除了不支持前向安全性的** TLS1.3 对密码套件进行“减肥”了， **RSA 和 DH 算法，只支持 ECDHE 算法** 。

对于对称加密和签名算法，只支持目前最安全的几个密码套件，比如 openssl 中仅支持下面 5 种密码套件：


#### TLS_AES_256_GCM_SHA384

#### TLS_CHACHA20_POLY1305_SHA256

#### TLS_AES_128_GCM_SHA256

#### TLS_AES_128_CCM_8_SHA256

#### TLS_AES_128_CCM_SHA256

之所以用降级攻击，伪造客户端的 TLS1.3 仅支持这么少的密码套件，是因为 Client Hello 消息，替换客户端支持的密码套件为一些不安全的密码套件，使得服务器 TLS1.2 由于支持各种古老且不安全的密码套件，中间人可以利
被迫使用这个密码套件进行 HTTPS 连接，从而破解密文。

## 证书优化^

#### 为了验证的服务器的身份，服务器会在的。 TSL 握手过程中，把自己的证书发给客户端，以此证明自己身份是可信

#### 对于证书的优化，可以有两个方向：

#### 一个是 证书传输 ，

#### 一个是 证书验证 ；

### 证书传输优化

#### 要让证书更便于传输，那必然是减少证书的大小，这样可以节约带宽，也能减少客户端的运算量。所以， 器的证书应该选择椭圆曲线（ECDSA）证书，而不是 RSA 证书，因为在相同安全强度下， ECC 密钥⻓度比对于服务 RSA

#### 短的多 。

### 证书验证优化

#### 客户端在验证证书时，是个复杂的过程，会走证书链逐级验证，验证的过程不仅需要「用「用签名算法验证证书的完整性」，而且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA 公钥解密证书」以及 CA， 下载 CRL

#### 或者 OCSP 数据，以此确认证书的有效性。

#### 这个访问过程是 HTTP 访问，因此又会产生一系列网络通信的开销，如 DNS 查询、建立连接、收发数据等。

#### CRL

CRL 证书序号，如果服务器的证书在此列表，就认为证书已经失效，不在的话，则认为证书是有效的。称为证书吊销列表（ _Certificate Revocation List_ ），这个列表是由 CA 定期更新，列表内容都是被撤销信任的


#### 但是 CRL 存在两个问题：

#### 第一个问题，由于 CRL 列表是由 CA 维护的，定期更新，如果一个证书刚被吊销后，客户端在更新 CRL 之

#### 前还是会信任这个证书， 实时性较差 ；

#### 第二个问题，的列表，那么就会导致客户端在校验证书这一环节的延时很大，进而拖慢了 随着吊销证书的增多，列表会越来越大，下载的速度就会越慢 ，下载完客户端还得遍历这么大 HTTPS 连接。

#### OCSP

因此，现在基本都是使用性，它的工作方式是 **向 CA** OCSP **发送查询请求，让** ，名为在线证书状态协议（ **CA 返回证书的有效状态** _Online Certificate Status Protocol_ 。 ）来查询证书的有效


#### 不必像决了 CRL CRL 的实时性问题。方式客户端需要下载大大的列表，还要从列表查询，同时因为可以实时查询每一张证书的有效性，解

#### OCSP CA 服务器繁忙，也会导致客户端在校验证书这一环节的延时变大。需要向 CA 查询，因此也是要发生网络请求，而且还得看 CA 服务器的“脸色”，如果网络状态不好，或者

**OCSP Stapling**
于是为了解决这一个网络开销，就出现了一个带有时间戳和签名的响应结果并缓存它。 OCSP Stapling，其原理是：服务器向 CA 周期性地查询证书状态，获得


#### 当有客户端发起连接请求时，服务器会把这个「响应结果」在服务器无法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询。 TLS 握手过程中发给客户端。由于有签名的存在，

## 会话复用^

#### TLS 密密钥缓存起来，待下次需要建立握手的目的就是为了协商出会话密钥，也就是对称加密密钥，那我们如果我们把首次 HTTPS 连接时，直接「复用」这个密钥，不就减少 TLS TLS 握手的性能损耗了吗？握手协商的对称加

这种方式就是 **会话复用** （ _TLS session resumption_ ），会话复用分两种：
第一种叫 Session ID；
第二种叫 Session Ticket；

### Session ID

Session ID **Session ID** 的工作原理是， **来标识** ，Session ID **客户端和服务器首次** 和会话密钥相当于 **TLS** key-value **握手连接后，双方会在内存缓存会话密钥，并用唯一的** 的关系。

当客户端再次连接时，钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信。当然为了安全性，内存中的会话密钥hello 消息里会带上 Session ID，服务器收到后就会从内存找，如果找到就直接用该会话密
会定期失效。


#### 但是它有两个缺点：

#### 服务器必须保持每一个客户端的会话密钥，随着客户端的增多， 服务器的内存压力也会越大 。

#### 现在网站服务一般是由多台服务器通过负载均衡提供服务的， 务器 ，于是还要走完整的 TLS 握手过程； 客户端再次连接不一定会命中上次访问过的服

### Session Ticket

为了解决 **作交给了客户端** Session ID ，类似于的问题，就出现了 HTTP 的 Cookie Session Ticket。 ， **服务器不再缓存每个客户端的会话密钥，而是把缓存的工**

客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。
客户端再次连接服务器时，客户端会发送如果没问题，就可以恢复会话了，开始加密通信。 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，


对于集群服务器的话，台服务器时，都能恢复会话。 **要确保每台服务器加密 「会话密钥」的密钥是一致的** ，这样客户端携带 Ticket 访问任意一

Session ID 话密钥」，前面劫持的通信密文都会被破解。和 Session Ticket **都不具备前向安全性** ，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会

同时应对 **重放攻击** 也很困难，这里简单介绍下重放攻击工作的原理。

假设如哈希函数的转换之后）。与此同时， Alice 想向 Bob 证明自己的身份。Eve Bob 窃听了对话并保留了密码（或哈希）。要求 Alice 的密码作为身份证明，爱丽丝应尽全力提供（可能是在经过

交换结束后，读取的 Alice 的密码（或哈希），从而授予Eve（冒充 Alice ）连接到 Bob Eve 。当被要求提供身份证明时，访问权限。 Eve 发送从 Bob 接受的最后一个会话中

重放攻击的危险之处在于，如果中间人截获了某个客户端的般 POST 请求会改变数据库的数据，中间人就可以利用此截获的报文，不断向服务器发送该报文，这样就会导致数 Session ID 或 Session Ticket 以及 POST 报文，而一
据库的数据被中间人改变了，而客户是不知情的。
避免重放攻击的方式就是需要 **对会话密钥设定一个合理的过期时间** 。


### Pre-shared Key

前面的 Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。
而和 TLS1.3 HTTP 请求一同发送给服务端，这种方式叫更为牛逼，对于重连 TLS1.3 只需要 **Pre-shared Key 0 RTT** ，原理和。 Ticket 类似，只不过在重连时，客户端会把 Ticket

同样的，Pre-shared Key 也有重放攻击的危险。


#### 如上图，假设中间人通过某种方式，截获了客户端使用会话重用技术的据库的数据，然后中间人就可以把截获的这个报文发送给服务器，服务器收到后，也认为是合法的，于是就恢复会 POST 请求，通常 POST 请求是会改变数

#### 话，致使数据库的数据又被更改，但是此时用户是不知情的。

#### 所以，应对重放攻击可以给会话密钥设定一个合理的过期时间，以及只针对安全的会话重用。 HTTP 请求如 GET/HEAD 使用

## 总结^

#### 对于硬件优化的方向，因为 特性的 CPU ，这个特性可以在硬件级别优化 HTTPS 是属于计算密集型，应该选择计算力更强的 AES 对称加密算法，加快应用数据的加解密。 CPU，而且最好选择 支持 AES-NI

对于软件优化的方向，如果可以，把软件升级成较新的版本，比如将升级到 1.1.1，因为新版本的软件不仅会提供新的特性，而且还会修复老版本的问题。 Linux 内核 2.X 升级成 4.X，将 openssl 1.0.1

对于协议优化的方向：
密钥交换算法应该选择以在第三次握手之后，就发送加密应用数据，节省了 **ECDHE 算法** ，而不用 RSA 算法，因为 1 RTT。 ECDHE 算法具备前向安全性，而且客户端可
将 TSL1.2 升级 **TSL1.3** ，因为 TSL1.3 的握手过程只需要 1 RTT，而且安全性更强。
对于证书优化的方向：
服务器应该选用这样可以提高证书传输的效率； **ECDSA 证书** ，而非 RSA 证书，因为在相同安全级别下，ECC 的密钥⻓度比 RSA 短很多，
服务器应该开启TLS 握手的时候就不用再访问 **OCSP Stapling** CA 功能，由服务器预先获得服务器，减少了网络通信的开销，提高了证书验证的效率； OCSP 的响应，并把响应结果缓存起来，这样


#### 对于重连复会话，而不用再重新走完整的 HTTPS 时，我们可以使用一些技术让客户端和服务端使用上一次 TLS 握手过程。 HTTPS 连接使用的会话密钥，直接恢

常⻅的就可以恢复会话。对于 **会话重用** 技术有 Session ID TLS1.3 使用和 Pre-shared Key Session Ticket，用了会话重用技术，当再次重连会话重用技术，只需要 0 RTT 就可以恢复会话。 HTTPS 时，只需要 1 RTT

这些会话重用技术虽然好用，但是存在一定的安全⻛险，它们不仅不具备前向安全，而且有重放攻击的⻛险，所以应当对会话密钥设定一个合理的过期时间。

#### 参考资料：

1. [http://www.doc88.com/p-8621583210895.html](http://www.doc88.com/p-8621583210895.html)
2. https://zhuanlan.zhihu.com/p/33685085
3. https://en.wikipedia.org/wiki/Replay_attack
4. https://en.wikipedia.org/wiki/Downgrade_attack
5. https://www.cnblogs.com/racent-Z/p/14011056.html
6. [http://www.guoyanbin.com/a-detailed-look-at-rfc-8446-a-k-a-tls-1-3/](http://www.guoyanbin.com/a-detailed-look-at-rfc-8446-a-k-a-tls-1-3/)
7. https://www.thesslstore.com/blog/crl-explained-what-is-a-certificate-revocation-list/

## 最后^

#### 哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！


## 2.6 HTTP/2 牛逼在哪？

#### 不多 BB 了，直接发⻋！

#### 一起来看看 HTTP/2 牛逼在哪？


## HTTP/1.1 协议的性能问题^

#### 我们得先要了解下 HTTP/1.1 协议存在的性能问题，因为 HTTP/2 协议就是把这些性能问题逐个攻破了。

#### 现在的站点相比以前变化太多了，比如：

#### 消息的大小变大了，从几 KB 大小的消息，到几 MB 大小的消息；

#### ⻚面资源变多了，从每个⻚面不到 10 个的资源，到每⻚超 100 多个资源；

#### 内容形式变多样了，从单纯到文本内容，到图片、视频、音频等内容；

#### 实时性要求变高了，对⻚面的实时性要求的应用越来越多；

#### 这些变化带来的最大性能问题就是 HTTP/1.1 的高延迟 ，延迟高必然影响的就是用户体验。主要原因如下几个：

#### 延迟难以下降白了就是到达了延迟的下限；，虽然现在网络的「带宽」相比以前变多了，但是延迟降到一定幅度后，就很难再下降了，说

#### 并发连接有限TCP 慢启动过程给流量带来的影响；，谷歌浏览器最大并发连接数是 6 个，而且每一个连接都要经过 TCP 和 TLS 握手耗时，以及

#### 队头阻塞问题，同一连接只能在完成一个 HTTP 事务（请求和响应）后，才能处理下一个事务；

```
HTTP cookie 头部巨大且重复的头部，而 cookie ，由于的大小通常很大； HTTP 协议是无状态的，每一个请求都得携带 HTTP 头部，特别是对于有携带不支持服务器推送消息了带宽和服务器资源。，因此当客户端需要获取通知时，只能通过定时器不断地拉取消息，这无疑浪费大量
```

为了解决 HTTP/1.1 性能问题，具体的优化手段你可以看这篇文章「」，这里我举例几个常⻅的优化手段：
将多张小图合并成一张大图供浏览器来了新的问题，当某张小图片更新了，那么需要重新请求大图片，浪费了大量的网络带宽； JavaScript 来切割使用，这样可以将多个请求合并成一个请求，但是带
将图片的二进制数据通过次数； base64 编码后，把编码数据嵌入到 HTML 或 CSS 文件中，以此来减少网络请求
将多个体积较小的求替代了很多个请求，但是带来的问题，当某个 JavaScript 文件使用 webpack js 等工具打包成一个体积更大的文件变化了，需要重新请求同一个包里的所有 JavaScript 文件，以一个请 js 文件；
将同一个⻚面的资源分散到不同域名，提升并发连接上限，因为浏览器通常对同一域名的能是 6 个； HTTP 连接最大只

尽管对 HTTP/1.1 协议的优化手段如此之多，但是效果还是不尽人意，因为这些手段都是对 HTTP/1.1 协议的“外
部 **能主动推送等，要改变这些必须重新设计** ”做优化， **而一些关键的地方是没办法优化的，比如请求 HTTP 协议，于是-响应模型、头部巨大且重复、并发连接耗时、服务器不 HTTP/2 就出来了！**

## 兼容 HTTP/1.1^

#### HTTP/2 协议推广起来就相当困难，所幸出来的目的是为了改善 HTTP HTTP/2 的性能。协议升级有一个很重要的地方，就是要做到了兼容 HTTP/1.1 。 兼容 老版本的协议，否则新


#### 那么，HTTP/2 是怎么做的呢？

第一点，于是只需要浏览器和服务器在背后自动升级协议，这样可以让用户意识不到协议的升级，很好的实现了协议的平滑HTTP/2 没有在 URI 里引入新的协议名，仍然用「http://」表示明文协议，用「https://」表示加密协议，
升级。

第二点，只在应用层做了改变，还是基于解成了「语义」和「语法」两个部分，「语义」层不做改动，与 TCP 协议传输，应用层方面为了保持功能上的兼容， HTTP/1.1 完全一致，比如请求方法、状态码、头HTTP/2 把 HTTP 分
字段等规则保留不变。
但是，HTTP/2 在「语法」层面做了很多改造，基本改变了 HTTP 报文的传输格式。

## 头部压缩^

HTTP Encoding协议的报文是由「」指定 Body 的压缩方式，比如用Header + Body」构成的，对于 gzip 压缩，这样可以节约带宽，但报文中的另外一部分 Body 部分，HTTP/1.1 协议可以使用头字段 Header 「Content-，是没
有针对它的优化手段。
HTTP/1.1 报文中 Header 部分存在的问题：
含很多固定的字段，比如所以有必要 **压缩** ； Cookie、User Agent、Accept 等，这些字段加起来也高达几百字节甚至上千字节，
大量的请求和响应的报文里有很多字段值都是重复的，这样会使得大量带宽被这些冗余的数据占用了，所以
有必须要 **避免重复性** ；
字段是 ASCII 编码的，虽然易于人类观察，但效率低，所以有必要改成 **二进制编码** ；
HTTP/2 对 Header 部分做了大改造，把以上的问题都解决了。
HTTP/2 分： 没使用常⻅的 gzip 压缩方式来压缩头部，而是开发了 **HPACK** 算法，HPACK 算法主要包含三个组成部

静态字典；
动态字典；
Huffman 编码（压缩算法）；
客户端和服务器两端都会建立和维护「数据， **可达到 50%~90% 的高压缩率** 。 **字典** 」，用⻓度较小的索引号表示重复的字符串，再用 Huffman 编码压缩


### 静态表编码

#### HTTP/2 表里共有为高频出现在头部的字符串和字段建立了一张 61 组，如下图： 静态表 ，它是写入到 HTTP/2 框架里的，不会变化的，静态


表中的如 Index Index为 2 代表 表示索引（ GET，Index Key），为 8 Header Value代表状态码 200 表示索引对应的。 Value，Header Name 表示字段的名字，比


你可能注意到，表中有的Value 都会经过 Huffman Index 编码后，才会发送出去。没有对应的 Header Value，这是因为这些 Value 并不是固定的而是变化的，这些

这么说有点抽象，我们来看个具体的例子，下面这个 server 头部字段，在 HTTP/1.1 的形式如下：

算上冒号空格和末尾的 **率大概 47 %** 。 \r\n，共占用了 17 字节， **而使用了静态表和 Huffman 编码，可以将它压缩成 8 字节，压缩**

我抓了个server HTTP/2 头部数据。协议的网络包，你可以从下图看到，高亮部分就是 server 头部字段，只用了 8 个字节来表示

根据 01 RFC7541 ，所以整个头部格式如下图：规范，如果头部字段属于静态表范围，并且 Value 是变化，那么它的 HTTP/2 头部前 2 位固定为

HTTP/2 Length）来分割头部由于基于 Index **二进制编码** 和 Value。，就不需要冒号空格和末尾的\r\n作为分隔符，于是改用表示字符串⻓度（Value

接下来，根据这个头部格式来分析上面抓包的 server 头部的二进制数据。

```
server: nghttpx\r\n
```

首先，从静态表中能查到字节就是 01110110 ，这正是上面抓包标注的红色部分的二进制数据。 server 头部字段的 Index 为 54 ，二进制为 110110 ，再加上固定 01 ，头部格式第 1 个

然后，第二个字节的首个比特位表示子的第二个字节为 10000110 ，首位比特位为 Value 是否经过 1 就代表 Huffman Value 编码，剩余的字符串是经过 (^7) Huffman 位表示 Value 编码的，经过的⻓度，比如这次例 Huffman 编
码的 Value ⻓度为 6 。
最后，字符串「较短」的编码表示，从而缩减字符串⻓度。 nghttpx 经过 Huffman 编码后压缩成了 6 个字节，Huffman 编码的原理是将高频出现的信息用
于是，在统计大量的RFC7541 文档找到这张 HTTP **静态** 头部后， **Huffman** HTTP/2 **表** ，我就不把表的全部内容列出来了，我只列出字符串根据出现频率将 ASCII 码编码为了 Huffman 编码表，可以在 nghttpx 中每个字
符对应的 Huffman 编码，如下图：
通过查表后，字符串同的颜色将他们对应起来了，最后的 nghttpx 的 Huffman 7 位是补位的。编码在下图看到，共 6 个字节，每一个字符的 Huffman 编码，我用相
最终，server 头部的二进制数据对应的静态头部格式如下：


### 动态表编码

静态表只包含了Index 从 62 起步，会在编码解码的时候随时更新。 61 种高频出现在头部的字符串，不在静态表范围内的头部字符串就要自行构建 **动态表** ，它的

比如，第一次发送时头部中的「和服务器双方都会更新自己的动态表，添加一个新的user-agent 」字段数据有上百个字节，经过 Index 号 62 。 **那么在下一次发送的时候，就不用重复发这个** Huffman 编码发送出去后，客户端
**字段的数据了，只用发 1 个字节的 Index 号就好了，因为双方都可以根据自己的动态表获取到字段的数据** 。
所以，使得动态表生效有一个前提：接上只发送了 1 次，或者重复传输时，字段总是略有变化，动态表就无法被充分利用了。 **必须同一个连接上，重复传输完全相同的 HTTP 头部** 。如果消息字段在 1 个连

因此，随着在同一最终每个头部字段都会变成 HTTP/2 连接上发送的报文越来越多，客户端和服务器双方的「字典」积累的越来越多，理论上 1 个字节的 Index，这样便避免了大量的冗余数据的传输，大大节约了带宽。


理想很美好，现实很⻣感。动态表越大，占用的内存也就越大，如果占用了太多内存，是会影响服务器性能的，因此 Web 服务器都会提供类似 http2_max_requests 的配置，用于限制一个连接上能够传输的请求数量，避免动态
表无限增大，请求数量到达上限后，就会关闭 HTTP/2 连接来释放内存。
综上，HTTP/2 头部的编码通过「静态表、动态表、Huffman 编码」共同完成的。

## 二进制帧^

#### HTTP/2 制数据使用位运算能高效解析。厉害的地方在于将 HTTP/1 的文本格式改成二进制格式传输数据，极大提高了 HTTP 传输效率，而且二进

#### 你可以从下图看到，HTTP/1.1 的响应 和 HTTP/2 的区别：

HTTP/2 也就是说一条把响应报文划分成了两个 HTTP 响应，划分成了两个帧来传输，并且采用二进制来编码。 **帧（** **_Frame_** **）** ，图中的 HEADERS（首部）和 DATA（消息负载） 是帧的类型，


#### HTTP/2 二进制帧 的结构如下图：

帧头（Fream Header）很小，只有 9 个字节，帧开头的前 3 个字节表示帧数据（Fream Playload）的 **⻓度** 。
帧⻓度后面的一个字节是表示下表格： **帧的类型** ，HTTP/2 总共定义了 10 种类型的帧，一般分为 **数据帧** 和 **控制帧** 两类，如

#### 帧类型后面的一个字节是 标志位 ，可以保存 8 个标志位，用于携带简单的控制信息，比如：


**END_HEADERS** 表示头数据结束标志，相当于 HTTP/1 里头后的空行（“\r\n”）；
**END_STREAM** 表示单方向数据发送结束，后续不会再有数据帧。
**PRIORITY** 表示流的优先级；
帧头的最后值是 2^31，大约是 4 个字节是 21 **流标识符** 亿，它的作用是用来标识该（Stream ID），但最高位被保留不用，只有 Fream 属于哪个 Stream，接收方可以根据这个信息从乱序的帧 31 位可以使用，因此流标识符的最大
里找到相同 Stream ID 的帧，从而有序组装信息。
最后面就是 **帧数据** 了，它存放的是通过 **HPACK 算法** 压缩过的 HTTP 头部和包体。

## 并发传输^

#### 知道了 HTTP/2 的帧结构后，我们再来看看它是如何实现 并发传输 的。

#### 我们都知道处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续 HTTP/1.1 的实现是基于请求-响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能

#### 的请求是无法发送的，也造成了 队头阻塞 的问题。

而HTTP/1.1 HTTP/2 队头阻塞的问题，提高了就很牛逼了，通过 Stream HTTP 这个设计，传输的吞吐量。 **多个 Stream 复用一条 TCP 连接，达到并发的效果** ，解决了

为了理解 HTTP/2 的并发是怎样实现的，我们先来理解 HTTP/2 中的 Stream、Message、Frame 这 3 个概念。


#### 你可以从上图中看到：

```
1 个 TCP 连接包含一个或者多个 Stream，Stream 是 HTTP/2 并发的关键技术；Stream 成； 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构Message 容（头部和包体）；里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内
```

因此，我们可以得出 2 个结论：HTTP 消息可以由多个 Frame 构成，以及 1 个 Frame 可以由多个 TCP 报文构成。
在 HTTP/2 连接上， **不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ）** ，因为每个帧的头部会
携带 **格有序的** Stream ID 。 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，而 **同一 Stream 内部的帧必须是严**


客户端和服务器务器建立的 Stream **双方都可以建立** 必须是偶数号。 **Stream** ， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服

同一个连接中的GOAWAY ，用来关闭 Stream ID TCP 是不能复用的，只能顺序递增，所以当连接。 Stream ID 耗尽时，需要发一个控制帧

在 Nginx 中，可以通过 http2_max_concurrent_streams 配置来设置 Stream 的上限，默认是 128 个。
HTTP/2 通过 Stream 实现的并发，比 HTTP/1.1 通过 TCP 连接实现并发要牛逼的多， **因为当 HTTP/2 实现 100 个
并发TCP Stream 握手、慢启动以及时，只需要建立一次 TLS 握手过程，这些都是很耗时的。 TCP 连接，而 HTTP/1.1 需要建立 100 个 TCP 连接，每个 TCP 连接都要经过**

HTTP/2 HTML/CSS 还可以对每个和图片资源时，希望服务器先传递 Stream 设置不同 **优先级** ，帧头中的「标志位」可以设置优先级，比如客户端访问 HTML/CSS，再传图片，那么就可以通过设置 Stream 的优先级来
实现，以此提高用户体验。

## 服务器主动推送资源^


#### HTTP/1.1 源。 不支持服务器主动推送资源给客户端，都是由客户端向服务器发起请求后，才能获取到服务器响应的资

#### 比如，客户端通过这时客户端还要再发起获取 HTTP/1.1 CSS 请求从服务器那获取到了文件的请求，需要两次消息往返，如下图左边部分： HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染⻚面，

#### 如上图右边部分，在的次数。 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递

在 Nginx 中，如果你希望客户端访问 /test.html 时，服务器直接推送 /test.css，那么可以这么配置：

#### 那 HTTP/2 的推送是怎么实现的？

客户端发起的请求，必须使用的是奇数号源时，会通过 PUSH_PROMISE 帧传输 Stream HTTP 头部，并通过帧中的，服务器主动的推送，使用的是偶数号 Promised Stream ID Stream 字段告知客户端，接。服务器在推送资
下来会在哪个偶数号 Stream 中发送包体。

location /test.html { http2_push /test.css; (^)
}


如上图，在是可以 **并发** Stream 1 的。 中通知客户端 CSS 资源即将到来，然后在 Stream 2 中发送 CSS 资源，注意 Stream 1 和 2

## 总结^


#### HTTP/2 协议其实还有很多内容，比如流控制、流状态、依赖关系等等。

#### 这次主要介绍了关于 HTTP/2 是如何提示性能的几个方向，它相比 HTTP/1 大大提高了传输效率、吞吐能力。

第一点，对于常⻅的 HTTP 头部通过 **静态表和 Huffman 编码** 的方式，将体积压缩了近一半，而且针对后续的请求
头部，还可以建立 **动态表** ，将体积压缩近 90%，大大提高了编码效率，同时节约了带宽资源。
不过，动态表并非可以无限增大，并发能力，因此服务器需要限制 HTTP/2 因为动态表是会占用内存的，动态表越大，内存也越大，容易影响服务器总体的连接时⻓或者请求次数。

第二点，及减少了 **HTTP/2** TCP 慢启动阶段对流量的影响。不同的 **实现了 Stream 并发** ，多个 Stream Stream ID 只需复用才可以并发，即时乱序发送帧也没问题，但是同一个 1 个 TCP 连接，节约了 TCP 和 TLS 握手时间，以
Stream 里的帧必须严格有序。
另外，可以根据资源的渲染顺序来设置 Stream 的 **优先级** ，从而提高用户体验。

第三点，帧，告诉客户端接下来在哪个 **服务器支持主动推送资源** Stream ，大大提升了消息的传输性能，服务器推送资源时，会先发送发送资源，然后用偶数号 Stream 发送资源给客户端。 PUSH_PROMISE

HTTP/2 阻塞”的问题，只不过问题不是在通过 Stream 的并发能力，解决了 HTTP 这一层面，而是在 HTTP/1 队头阻塞的问题，看似很完美了，但是 TCP 这一层。 HTTP/2 还是存在“队头

**HTTP/2 这样内核才会将缓冲区里的数据返回给是基于 TCP 协议来传输数据的， HTTP TCP 应用，那么当「前是字节流协议，TCP 1 个字节数据」没有到达时，后收到的字节数据只层必须保证收到的字节数据是完整且连续的，
能存放在内核缓冲区里，只有等到这HTTP/2 队头阻塞问题。 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是**

有没有什么解决方案呢？既然是议，这个大胆的决定， HTTP/3 TCP 协议做了！协议自身的问题，那干脆放弃 TCP 协议，转而使用 UDP 协议作为传输层协


#### 参考资料：

1. https://developers.google.com/web/fundamentals/performance/http2
2. https://http2.akamai.com/demo
3. https://tools.ietf.org/html/rfc7541

## 最后^

#### 哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！

## 2.7 HTTP/3 强势来袭

#### HTTP/3 对于包格式可能后续会有变化。现在还没正式推出，不过自 2017 年起， HTTP/3 已经更新到 34 个草案了，基本的特性已经确定下来了，

#### 所以，这次 HTTP/3 介绍不会涉及到包格式，只说它的特性。


## 美中不足的 HTTP/2^

#### HTTP/2 足的是 HTTP/2 通过头部压缩、二进制编码、多路复用、服务器推送等新特性大幅度提升了协议是基于 TCP 实现的，于是存在的缺陷有三个。 HTTP/1.1 的性能，而美中不

#### 队头阻塞；

#### TCP 与 TLS 的握手时延迟；

#### 网络迁移需要重新连接；

### 队头阻塞

#### HTTP/2 TCP 连接中的所有请求。多个请求是跑在一个 TCP 连接中的，那么当 TCP 丢包时，整个 TCP 都要等待重传，那么就会阻塞该

#### 因为中丢失了，即使序列号较高的 TCP 是字节流协议，TCP TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的段已经被接收了，应用层也无法从内核中读取到这部分数据，从 TCP HTTP 段在网络传输视⻆

#### 看，就是请求被阻塞了。

#### 举个例子，如下图：


图中发送方发送了很多个网络中丢失了，即使 packet 4-6 packet，每个被接收方收到后，由于内核中的 packet 都有自己的序号，你可以认为是 TCP 数据不是连续的，于是接收方的应用层就无 TCP 的序列号，其中 packet 3 在
法从内核中读取到，只有等到队头阻塞问题，是在 TCP 层面发生的。 packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的

### TCP 与 TLS 的握手时延迟

#### 发起能发出请求数据。 HTTP 请求时，需要经过 TCP 三次握手和 TLS 四次握手（TLS 1.2）的过程，因此共需要 3 个 RTT 的时延才


#### 另外，产生"减速 TCP "效果。由于具有「拥塞控制」的特性，所以刚建立连接的 TCP 会有个「慢启动」的过程，它会对 TCP 连接

### 网络迁移需要重新连接

#### 一个口变动了，就会导致需要 TCP 连接是由四元组（源 TCP 与 IP TLS 地址，源端口，目标重新握手，这不利于移动设备切换网络的场景，比如 IP 地址，目标端口）确定的，这意味着如果 4G 网络环境切换成 IP 地址或者端

#### WIFI。

#### 这些问题都是 传输层协议替换成 TCP UDP 协议固有的问题，无论应用层的，这个大胆的决定，HTTP/3 HTTP/2 做了！在怎么设计都无法逃脱。要解决这个问题，就必须把


## QUIC 协议的特点^

#### 我们深知，UDP 是一个简单、不可靠的传输协议，而且是 UDP 包之间是无序的，也没有依赖关系。

#### 而且，UDP 是不需要连接的，也就不需要握手和挥手的过程，所以天然的就比 TCP 快。

#### 当然，具有类似HTTP/3 TCP 的连接管理、拥塞窗口、流量控制的网络特性，相当于将不可靠传输的不仅仅只是简单将传输协议替换成了 UDP，还基于 UDP 协议在「应用层」实现了 UDP 协议变成 QUIC “可靠 协议 ”的了，，它

#### 所以不用担心数据包丢失的问题。

#### QUIC 协议的优点有很多，这里举例几个，比如：

#### 无队头阻塞；

#### 更快的连接建立；

#### 连接迁移；

### 无队头阻塞

QUIC 可以认为就是一条协议也有类似 HTTP HTTP/2 Stream 请求。 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream

由于 QUIC 使用的传输协议是 UDP，UDP 不关心数据包的顺序，如果数据包丢失，UDP 也不关心。
不过使该流的其他数据包到达了，数据也无法被 QUIC 协议会保证数据包的可靠性，每个数据包都有一个序号唯一标识。当某个流中的一个数据包丢失了，即 HTTP/3 读取，直到 QUIC 重传丢失的报文，数据才会交给 HTTP/3。


#### 而其他流的数据报文只要被完整接收，数据包丢失了，其他流也会因此受影响。HTTP/3 就可以读取到数据。这与 HTTP/2 不同，HTTP/2 只要某个流中的

所以，受影响。QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不

### 更快的连接建立

对于此它们难以合并在一起，需要分批次来握手，先 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、 TCP 握手，再 TLS 握手。 openssl 库实现的表示层，因

HTTP/3 ID」，连接迁移就是基于连接在传输数据前虽然需要 ID QUIC 实现的。协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接

但是 **录”，再加上** HTTP/3 **QUIC** 的 QUIC **使用的是** 协议并不是与 **TLS1.3** TLS **，因此仅需** 分层，而是 **1 个 RTT QUIC 就可以「同时」完成建立连接与密钥协商，甚至在第二内部包含了 TLS，它在自己的帧会携带 TLS 里的“记
次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果** 。
如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT：


### 连接迁移

#### 在前面我们提到，基于定一条 TCP 连接，那么当移动设备的网络从 TCP 传输协议的 HTTP 4G 协议，由于是通过四元组（源切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连 IP、源端口、目的 IP、目的端口）确

#### 接，然后重新建立连接，而建立连接的过程包含过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速

#### 而自选择一组 QUIC 协议没有用四元组的方式来 ID 来标记自己，因此即使移动设备的网络变化后，导致“绑定”连接，而是通过 连接 ID 来标记通信的两个端点，客户端和服务器可以各 IP 地址变化了，只要仍保有上下文信息（比如

#### 连接能。 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了 连接迁移 的功

## HTTP/3 协议^

#### 了解完 QUIC 协议的特点后，我们再来看看 HTTP/3 协议在 HTTP 这一层做了什么变化。

HTTP/3 HTTP/3 自身不需要再定义同 HTTP/2 一样采用二进制帧的结构，不同的地方在于 Stream，直接使用 QUIC 里的 Stream HTTP/2 ，于是 HTTP/3 的二进制帧里需要定义的帧的结构也变简单了。 Stream，而


#### 从上图可以看到，HTTP/3 帧头只有两个字段：类型和⻓度。

#### 根据帧类型的不同，大体上分为数据帧和控制帧两大类，属于数据帧。 HEADERS 帧（HTTP 头部）和 DATA 帧（HTTP 包体）

HTTP/3 HTTP/3 在头部压缩算法这一方便也做了升级，升级成了中的 QPACK 也采用了静态表、动态表及 Huffman **QPACK** 编码。。与 HTTP/2 中的 HPACK 编码方式相似，

对于静态表的变化，HTTP/2 中的 HPACK 的静态表只有 61 项，而 HTTP/3 中的 QPACK 的静态表扩大到 91 项。
HTTP/2 和 HTTP/3 的 Huffman 编码并没有多大不同，但是动态表编解码方式不同。
所谓的动态表，在首次请求仅用 1 个数字表示，然后对方可以根据这-响应后，双方会将未包含在静态表中的 1 个数字从动态表查到对应的数据，就不必每次都传输⻓⻓的数据，大大 Header 项更新各自的动态表，接着后续传输时
提升了编码效率。

可以看到， **HPACK 头部，因为对方还没建立好动态表，因此后续的请求解码会阻塞到首次请求中丢失的数据包重传过来动态表是具有时序性的，如果首次出现的请求发生了丢包，后续的收到请求，对方就无法解码出** (^) 。
HTTP/3 的 QPACK 解决了这一问题，那它是如何解决的呢？
QUIC 消息时用的是双向流，这两个单向流的用法：会有两个特殊的单向流，所谓的单项流只有一端可以发送消息，双向则指两端都可以发送消息，传输 HTTP
一个叫HTTP 请求头部，客户端可以通过这个 QPACK Encoder Stream， 用于将一个字典（ Stream 发送字典；key-value）传递给对方，比如面对不属于静态表的
一个叫就可以使用这个字典来编码了。 QPACK Decoder Stream，用于响应对方，告诉它刚发的字典已经更新到自己的本地动态表了，后续
这两个特殊的单向流是用来部。 **同步双方的动态表** ，编码方收到解码方更新确认的通知后，才使用动态表编码 HTTP 头

## 总结^


#### HTTP/2 虽然具有多个流并发传输的能力，但是传输层是 TCP 协议，于是存在以下缺陷：

#### 队头阻塞 列号较高的，HTTP/2 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，从多个请求跑在一个 TCP 连接中，如果序列号较低的 TCP 段在网络传输中丢失了，即使序 HTTP 视⻆看，就是多个请

#### 求被阻塞了；

#### TCP 和 TLS 握手时延 ，TCL 三次握手和 TLS 四次握手，共有 3-RTT 的时延；

#### 连接迁移需要重新连接 连接的，那么网络环境变化后，就会导致，移动设备从 4G 网络环境切换到 IP 地址或端口变化，于是 WIFI 时，由于 TCP TCP 只能断开连接，然后再重新建立连是基于四元组来确认一条 TCP

#### 接，切换网络环境的成本高；

#### HTTP/3 就将传输层从 TCP 替换成了 UDP，并在 UDP 协议上开发了 QUIC 协议，来保证数据的可靠传输。

#### QUIC 协议的特点：

**无队头阻塞** 生丢包了，只会影响该流，其他流不受影响；，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，也不会有底层协议限制，某个流发
**建立连接速度快** 钥协商，甚至在第二次连接的时候，应用数据包可以和，因为 QUIC 内部包含 TLS1.3，因此仅需 QUIC 1 个握手信息（连接信息 RTT 就可以「同时」完成建立连接与 + TLS 信息）一起发送， TLS 密
达到 0-RTT 的效果。
**连接迁移** 户端和服务器可以各自选择一组，QUIC 协议没有用四元组的方式来 ID 来标记自己，因此即使移动设备的网络变化后，导致“绑定”连接，而是通过「连接 ID 」来标记通信的两个端点，客 IP 地址变化了，只
要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本；
另外 HTTP/3 的 QPACK 通过两个特殊的单向流来同步双方的动态表，解决了 HTTP/2 的 HPACK 队头阻塞问题。
**期待，HTTP/3 正式推出的那一天！**

参考资料：

1. https://medium.com/faun/http-2-spdy-and-http-3-quic-bae7d9a3d484
2. https://developers.google.com/web/fundamentals/performance/http2?hl=zh-cn
3. https://blog.cloudflare.com/http3-the-past-present-and-future/
4. https://tools.ietf.org/html/draft-ietf-quic-http-34
5. https://tools.ietf.org/html/draft-ietf-quic-transport-34#section-17
6. https://ably.com/topic/http3?amp%3Butm_campaign=evergreen&%3Butm_source=reddit&utm_medium=referral
7. https://www.nginx.org.cn/article/detail/422
8. https://www.bilibili.com/read/cv793000/
9. https://www.chinaz.com/2020/1009/1192436.shtml